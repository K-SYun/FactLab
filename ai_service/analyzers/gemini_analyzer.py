# Gemini AI ë¶„ì„ê¸° (í†µí•© ë²„ì „)

import os
import requests
import json
import logging
import psycopg2
from psycopg2.extras import RealDictCursor
from typing import Optional, Dict
from datetime import datetime
from dotenv import load_dotenv
from config.prompt_loader import get_prompt_loader

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GeminiNewsAnalyzer:
    def __init__(self):
        # Gemini API ì„¤ì •
        self.api_key = os.getenv("GEMINI_API_KEY")
        # Gemini API - gemini-pro ëª¨ë¸ ì‚¬ìš©
        self.api_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-pro-latest:generateContent"
        
        # ë°±ì—”ë“œ API ì—”ë“œí¬ì¸íŠ¸
        self.backend_url = os.getenv('BACKEND_URL', 'http://backend-service:8080')
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
        self.db_url = os.getenv("DATABASE_URL", "postgresql://factlab_user:password@database:5432/factlab")
        
        if not self.api_key:
            logger.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Fallback ë¶„ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.enabled = False
        else:
            self.enabled = True
            logger.info("Gemini API ë¶„ì„ê¸°ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_db_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        return psycopg2.connect(self.db_url, cursor_factory=RealDictCursor)
    
    def analyze_news_content(self, title: str, content: str, source: str = "ì¶œì²˜ ë¶ˆëª…", analysis_type: str = "COMPREHENSIVE") -> Dict:
        """ë‰´ìŠ¤ ë‚´ìš© ì¢…í•© ë¶„ì„"""
        if not self.enabled:
            return self.generate_fallback_analysis(title, content)
        
        try:
            logger.info(f"Gemini AI ë¶„ì„ ì‹œì‘ - ì œëª©: {title[:50]}..., íƒ€ì…: {analysis_type}")
            
            # YAML íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            prompt_loader = get_prompt_loader()
            
            # source ë§¤ê°œë³€ìˆ˜ë¥¼ ì§ì ‘ ì‚¬ìš©
            
            prompt = prompt_loader.build_prompt(analysis_type, title, content, source)
            
            headers = {"Content-Type": "application/json"}
            payload = {
                "contents": [{
                    "parts": [{"text": prompt}]
                }]
            }
            
            # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ (503 ì˜¤ë¥˜ ëŒ€ì‘)
            max_retries = 3
            retry_delay = 2

            for attempt in range(max_retries):
                try:
                    logger.info(f"Attempting to call Gemini API ({attempt + 1}/{max_retries})...")
                    start_time = time.time()
                    response = requests.post(
                        f"{self.api_url}?key={self.api_key}",
                        headers=headers,
                        data=json.dumps(payload),
                        timeout=60
                    )
                    duration = time.time() - start_time
                    logger.info(f"Gemini API responded in {duration:.2f} seconds.")

                    # 503 ì˜¤ë¥˜ê°€ ì•„ë‹ˆë©´ ì¦‰ì‹œ ì²˜ë¦¬
                    if response.status_code != 503:
                        break

                    # 503 ì˜¤ë¥˜ì¸ ê²½ìš° ì¬ì‹œë„
                    if attempt < max_retries - 1:
                        logger.warning(f"Gemini API 503 ì˜¤ë¥˜, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                        import time
                        time.sleep(retry_delay)
                        retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    else:
                        logger.error(f"Gemini API ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {response.status_code}")

                except requests.exceptions.Timeout:
                    if attempt < max_retries - 1:
                        logger.warning(f"Gemini API íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                        import time
                        time.sleep(retry_delay)
                    else:
                        logger.error("Gemini API íƒ€ì„ì•„ì›ƒ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                        return self.generate_fallback_analysis(title, content)
            
            if response.status_code == 200:
                result = response.json()
                analysis_text = result["candidates"][0]["content"]["parts"][0]["text"].strip()

                # ë””ë²„ê¹…ìš© ì‘ë‹µ ë¡œê¹…
                logger.info(f"Gemini API ì›ë³¸ ì‘ë‹µ: {analysis_text[:500]}...")

                # JSON íŒŒì‹± ì‹œë„
                try:
                    # JSON ë¸”ë¡ ì¶”ì¶œ (```jsonìœ¼ë¡œ ê°ì‹¸ì§„ ê²½ìš° ì²˜ë¦¬)
                    if "```json" in analysis_text:
                        analysis_text = analysis_text.split("```json")[1].split("```")[0].strip()
                    elif "```" in analysis_text:
                        analysis_text = analysis_text.split("```")[1].split("```")[0].strip()

                    logger.info(f"JSON íŒŒì‹± ì‹œë„: {analysis_text[:200]}...")
                    analysis_result = json.loads(analysis_text)
                    logger.info(f"Gemini AI ë¶„ì„ ì™„ë£Œ - ì‹ ë¢°ë„: {analysis_result.get('reliability_score', 'N/A')}")
                    return analysis_result
                    
                except json.JSONDecodeError as e:
                    logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ë¶„ì„ ì‚¬ìš©: {e}")
                    return self.parse_text_analysis(analysis_text, title, content)
            
            else:
                logger.error(f"Gemini API ì‘ë‹µ ì˜¤ë¥˜: {response.status_code} {response.text}")
                return self.generate_fallback_analysis(title, content)
        
        except Exception as e:
            logger.error(f"Gemini ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return self.generate_fallback_analysis(title, content)
    
    def parse_text_analysis(self, text: str, title: str, content: str) -> Dict:
        """í…ìŠ¤íŠ¸ í˜•íƒœ ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì‹±"""
        try:
            # ê¸°ë³¸ê°’ ì„¤ì •
            result = {
                "summary": text[:200] + "..." if len(text) > 200 else text,
                "claim": "AI ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
                "keywords": self.extract_keywords_from_content(title + " " + content),
                "reliability_score": 75,
                "suspicious_points": "ì¶”ê°€ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤."
            }
            
            return result
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return self.generate_fallback_analysis(title, content)
    
    def extract_keywords_from_content(self, text: str) -> str:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # í•œêµ­ì–´ ë¶ˆìš©ì–´
        stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ë¡œ', 'ì™€', 'ê³¼', 'í•œ', 'í• ', 'í•˜ëŠ”', 'í•˜ë©°', 'ìˆëŠ”', 'ìˆë‹¤', 'ë•Œë¬¸ì—', 'í†µí•´', 'ìœ„í•´', 'ëŒ€í•œ', 'ê´€í•œ', 'ê°™ì€', 'ë”°ë¥¸', 'ìœ„í•œ'}
        
        words = []
        for word in text.split():
            if len(word) > 1 and word not in stopwords:
                words.append(word)
        
        # ë¹ˆë„ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ 5ê°œ ì¶”ì¶œ (ê°„ë‹¨ êµ¬í˜„)
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        keywords = [word[0] for word in top_words]
        
        return ','.join(keywords) if keywords else 'í‚¤ì›Œë“œ,ë¶„ì„,ë‰´ìŠ¤,ì •ë³´,ë‚´ìš©'
    
    def generate_fallback_analysis(self, title: str, content: str) -> Dict:
        """Gemini API ì‹¤íŒ¨ ì‹œ fallback ë¶„ì„"""
        logger.info("Fallback ë¶„ì„ ëª¨ë“œ ì‚¬ìš©")
        
        # ì¹´í…Œê³ ë¦¬ ê°ì§€
        category_keywords = {
            "ì •ì¹˜": ["ì •ë¶€", "êµ­íšŒ", "ëŒ€í†µë ¹", "ì •ì±…", "ë²•ì•ˆ", "ì„ ê±°", "ì •ì¹˜", "ë‹¹", "ì˜ì›"],
            "ê²½ì œ": ["ê²½ì œ", "ì‹œì¥", "ì£¼ì‹", "ê¸ˆë¦¬", "íˆ¬ì", "ê¸°ì—…", "ì¦ì‹œ", "ì‹¤ì ", "ë§¤ì¶œ", "ìˆ˜ìµ"],
            "ì‚¬íšŒ": ["ì‚¬íšŒ", "êµìœ¡", "ë³µì§€", "ì•ˆì „", "ë¬¸í™”", "ì‹œë¯¼", "ì§€ì—­", "ì£¼ë¯¼", "ìƒí™œ"],
            "ê¸°ìˆ ": ["ê¸°ìˆ ", "IT", "AI", "ê³¼í•™", "ì—°êµ¬", "ê°œë°œ", "í˜ì‹ ", "ë””ì§€í„¸", "ìŠ¤ë§ˆíŠ¸"],
            "ì—°ì˜ˆ": ["ì—°ì˜ˆ", "ë°°ìš°", "ê°€ìˆ˜", "ë“œë¼ë§ˆ", "ì˜í™”", "ë°©ì†¡", "ìŒì•…", "ì˜ˆëŠ¥"]
        }
        
        detected_category = "ì¼ë°˜"
        max_matches = 0
        text_lower = (title + " " + content).lower()
        
        for category, keywords in category_keywords.items():
            matches = sum(1 for keyword in keywords if keyword in text_lower)
            if matches > max_matches:
                max_matches = matches
                detected_category = category
        
        # ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        reliability = 70
        if len(content) > 500:
            reliability += 10
        if max_matches > 2:
            reliability += 5
        
        # ì œëª©ê³¼ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ìš”ì•½ ìƒì„± (ë³¸ë¬¸ ë³µì‚¬ ê¸ˆì§€)
        # ìš”ì•½ì€ ì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„± (ìµœëŒ€ 150ì)
        summary_templates = [
            f"{detected_category} ë¶„ì•¼: {title[:100]}{'...' if len(title) > 100 else ''}ì— ê´€í•œ ë³´ë„ì…ë‹ˆë‹¤.",
            f"ì´ ê¸°ì‚¬ëŠ” {detected_category} ê´€ë ¨ '{title[:80]}{'...' if len(title) > 80 else ''}'ë¥¼ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.",
            f"{detected_category} ìµœì‹  ë™í–¥: {title[:100]}{'...' if len(title) > 100 else ''}"
        ]

        # ì œëª© í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ í…œí”Œë¦¿ ì„ íƒ (ì¼ê´€ì„± ìœ ì§€)
        template_index = hash(title) % len(summary_templates)
        selected_summary = summary_templates[template_index]

        # ë‹¤ì–‘í•œ ì£¼ì¥ íŒ¨í„´
        claim_templates = [
            f"ê¸°ì‚¬ì—ì„œëŠ” {detected_category} ë¶„ì•¼ì˜ í˜„í™©ê³¼ ì „ë§ì— ëŒ€í•´ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.",
            f"ì´ ë³´ë„ëŠ” {detected_category} ê´€ë ¨ ì •ì±…ì´ë‚˜ ì‚¬ì•ˆì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            f"ì£¼ìš” ê´€ê³„ìë“¤ì˜ ì…ì¥ê³¼ {detected_category} ë¶„ì•¼ì˜ í–¥í›„ ë°©í–¥ì— ëŒ€í•œ ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        ]

        claim_index = hash(content[:100]) % len(claim_templates) if content else 0
        selected_claim = claim_templates[claim_index]

        return {
            "summary": selected_summary,
            "claim": selected_claim,
            "keywords": self.extract_keywords_from_content(title + " " + content),
            "reliability_score": min(reliability, 85),
            "suspicious_points": "AI ë¶„ì„ì´ ì¼ì‹œì ìœ¼ë¡œ ì œí•œë˜ì–´ ê¸°ë³¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤. ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ì¬ë¶„ì„ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        }
    
    def save_analysis_to_db(self, news_id: int, analysis: Dict, analysis_type: str = "COMPREHENSIVE", summary_id: int = None) -> bool:
        """AI ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥ (ìˆ˜ì •ëœ ë¡œì§)"""
        conn = None
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            # summary_idê°€ ì œê³µëœ ê²½ìš° í•´ë‹¹ ë ˆì½”ë“œ ì§ì ‘ ì‚¬ìš©
            if summary_id:
                logger.info(f"ì œê³µëœ summary_id ì‚¬ìš©: {summary_id}")
                cursor.execute("SELECT id FROM news_summary WHERE id = %s", (summary_id,))
                record_to_update = cursor.fetchone()
                if not record_to_update:
                    logger.error(f"ì œê³µëœ summary_id({summary_id})ì— í•´ë‹¹í•˜ëŠ” ë ˆì½”ë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                    return False
                update_summary_id = summary_id
            else:
                # ê¸°ì¡´ ë¡œì§: PENDING ìƒíƒœì˜ íŠ¹ì • ë¶„ì„ íƒ€ì… ë ˆì½”ë“œ ì°¾ê¸°
                logger.info(f"DBì—ì„œ ì—…ë°ì´íŠ¸í•  ë ˆì½”ë“œ ê²€ìƒ‰: news_id={news_id}, analysis_type={analysis_type}")
                cursor.execute("SELECT id FROM news_summary WHERE news_id = %s AND analysis_type = %s AND status = 'PENDING' ORDER BY created_at DESC LIMIT 1",
                               (news_id, analysis_type))
                record_to_update = cursor.fetchone()

                if not record_to_update:
                    logger.error(f"ì—…ë°ì´íŠ¸í•  PENDING ìƒíƒœì˜ ìš”ì•½ ë ˆì½”ë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: news_id={news_id}, type={analysis_type}")
                    # PENDINGì´ ì—†ìœ¼ë©´, ê·¸ëƒ¥ news_idì™€ typeìœ¼ë¡œ ì°¾ì•„ë³¸ë‹¤ (ì¬ë¶„ì„ì˜ ê²½ìš°)
                    cursor.execute("SELECT id FROM news_summary WHERE news_id = %s AND analysis_type = %s ORDER BY created_at DESC LIMIT 1",
                                   (news_id, analysis_type))
                    record_to_update = cursor.fetchone()
                    if not record_to_update:
                        logger.error(f"ì¬ë¶„ì„ì„ ìœ„í•œ ìš”ì•½ ë ˆì½”ë“œë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: news_id={news_id}, type={analysis_type}")
                        return False

                update_summary_id = record_to_update['id']
            logger.info(f"ë ˆì½”ë“œ ì°¾ìŒ (ID: {update_summary_id}). ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")

            # ê³µí†µ í•„ë“œ ì¶”ì¶œ
            summary_data = analysis.get('summary', '')
            # summaryê°€ dictë‚˜ listì¸ ê²½ìš° ì²˜ë¦¬
            if isinstance(summary_data, dict):
                summary = summary_data.get('text', summary_data.get('content', str(summary_data)))
            elif isinstance(summary_data, list) and summary_data:
                summary = ' '.join(str(item) for item in summary_data)
            else:
                summary = str(summary_data) if summary_data else ''

            # claim ì¶”ì¶œ (dict ë˜ëŠ” strì¼ ìˆ˜ ìˆìŒ)
            main_claim_data = analysis.get('main_claim', analysis.get('claim', ''))
            if isinstance(main_claim_data, dict):
                # dictì¸ ê²½ìš° 'claim' í‚¤ì˜ ê°’ë§Œ ì¶”ì¶œ
                claim = main_claim_data.get('claim', '')
                if not claim:
                    # 'claim' í‚¤ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ê°’ì„ ì‚¬ìš©
                    claim = next((v for v in main_claim_data.values() if isinstance(v, str)), str(main_claim_data))
            elif isinstance(main_claim_data, list) and main_claim_data:
                # listì¸ ê²½ìš° ì²« ë²ˆì§¸ í•­ëª© ì‚¬ìš©
                first_item = main_claim_data[0]
                claim = first_item.get('claim', str(first_item)) if isinstance(first_item, dict) else str(first_item)
            else:
                claim = str(main_claim_data) if main_claim_data else ''
            raw_keywords = analysis.get('keywords', [])
            if isinstance(raw_keywords, list):
                keywords = ','.join(map(str, raw_keywords))
            elif isinstance(raw_keywords, str):
                keywords = raw_keywords
            else:
                keywords = ''

            # reliability_score ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
            # ìš°ì„ ìˆœìœ„: reliability_score > credibility.overall_score > credibility.score > ê¸°ë³¸ê°’ 75
            reliability_score = None

            # 1. ì§ì ‘ reliability_score í‚¤ í™•ì¸
            if 'reliability_score' in analysis:
                reliability_score = analysis.get('reliability_score')

            # 2. credibility ê°ì²´ ë‚´ë¶€ í™•ì¸
            if reliability_score is None:
                credibility_data = analysis.get('credibility')
                if isinstance(credibility_data, dict):
                    # overall_score ë˜ëŠ” score ì‚¬ìš©
                    reliability_score = credibility_data.get('overall_score') or credibility_data.get('score')
                elif isinstance(credibility_data, (int, float)):
                    reliability_score = credibility_data

            # 3. ì •ìˆ˜ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
            if isinstance(reliability_score, (int, float)):
                reliability_score = int(reliability_score)
            else:
                reliability_score = 75
            suspicious_points_list = []
            
            # 1. Handle Fact Analysis part (present in FACT_ANALYSIS and COMPREHENSIVE)
            fact_analysis = analysis.get('fact_analysis')
            if fact_analysis and isinstance(fact_analysis, dict):
                questionable_claims = fact_analysis.get('questionable_claims')
                if questionable_claims and isinstance(questionable_claims, list):
                    for questionable_claim in questionable_claims:
                        if isinstance(questionable_claim, dict) and 'reason' in questionable_claim:
                            reason_item = questionable_claim['reason']
                            if isinstance(reason_item, list):
                                suspicious_points_list.append('. '.join(map(str, reason_item)))
                            else:
                                suspicious_points_list.append(str(reason_item))
                
                # Use overall assessment if no specific questionable claims
                if not questionable_claims:
                    assessment = fact_analysis.get('overall_assessment')
                    if assessment:
                        suspicious_points_list.append(assessment)

            # 2. Handle Bias Analysis part (present in BIAS_ANALYSIS and COMPREHENSIVE)
            bias_analysis = analysis.get('bias_analysis')
            if bias_analysis and isinstance(bias_analysis, dict):
                # political_leaning_reason ì²˜ë¦¬
                reason = bias_analysis.get('political_leaning_reason')
                if reason:
                    if isinstance(reason, list):
                        suspicious_points_list.append('. '.join(map(str, reason)))
                    else:
                        suspicious_points_list.append(str(reason))
                
                # framing_analysis ì²˜ë¦¬
                framing = bias_analysis.get('framing_analysis')
                if framing:
                    if isinstance(framing, list):
                        suspicious_points_list.append('. '.join(map(str, framing)))
                    else:
                        suspicious_points_list.append(str(framing))

            # 3. Handle Credibility part (present in COMPREHENSIVE)
            credibility_analysis = analysis.get('credibility')
            if credibility_analysis and isinstance(credibility_analysis, dict):
                reason = credibility_analysis.get('assessment_reason')
                if reason:
                    if isinstance(reason, list):
                        # If the reason is a list of bullet points, join them into a single string
                        suspicious_points_list.append('. '.join(map(str, reason)))
                    else:
                        # Otherwise, append the reason as is
                        suspicious_points_list.append(str(reason))

            # 4. Final fallback: use main summary if no suspicious points found
            if not suspicious_points_list:
                summary = analysis.get('summary')
                if summary:
                    suspicious_points_list.append("AI ìš”ì•½: " + summary)

            suspicious_points = '. '.join(suspicious_points_list)
            if suspicious_points and not suspicious_points.endswith('.'):
                suspicious_points += '.'


            # ì „ì²´ ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
            full_analysis_result = json.dumps(analysis, ensure_ascii=False, indent=2)

            # ë””ë²„ê¹…: ëª¨ë“  ë³€ìˆ˜ íƒ€ì… í™•ì¸
            logger.info(f"ğŸ“Š DB ì €ì¥ ì „ íƒ€ì… í™•ì¸:")
            logger.info(f"  summary: {type(summary).__name__}")
            logger.info(f"  claim: {type(claim).__name__}")
            logger.info(f"  keywords: {type(keywords).__name__}")
            logger.info(f"  reliability_score: {type(reliability_score).__name__}")
            logger.info(f"  suspicious_points: {type(suspicious_points).__name__}")

            # dict íƒ€ì…ì´ ìˆëŠ”ì§€ í•œë²ˆ ë” í™•ì¸í•˜ê³  ìë™ ë³€í™˜
            params_to_check = {
                'summary': summary,
                'claim': claim,
                'keywords': keywords,
                'suspicious_points': suspicious_points
            }

            for key, value in params_to_check.items():
                if isinstance(value, (dict, list)):
                    logger.error(f"âš ï¸ {key}ê°€ {type(value).__name__} íƒ€ì…ì…ë‹ˆë‹¤!")
                    logger.error(f"  ë‚´ìš©: {str(value)[:200]}")
                    # ìë™ ë³€í™˜
                    if isinstance(value, dict):
                        params_to_check[key] = json.dumps(value, ensure_ascii=False)
                    elif isinstance(value, list):
                        params_to_check[key] = ', '.join(str(item) for item in value)
                    logger.info(f"  â†’ ë¬¸ìì—´ë¡œ ë³€í™˜ ì™„ë£Œ")

            # ë³€í™˜ëœ ê°’ ì ìš©
            summary = params_to_check['summary']
            claim = params_to_check['claim']
            keywords = params_to_check['keywords']
            suspicious_points = params_to_check['suspicious_points']

            # ì—…ë°ì´íŠ¸ ì‹¤í–‰
            cursor.execute("""
                UPDATE news_summary
                SET summary = %s, claim = %s, keywords = %s,
                    reliability_score = %s, ai_confidence = %s,
                    updated_at = %s, status = 'COMPLETED',
                    full_analysis_result = %s, suspicious_points = %s
                WHERE id = %s
            """, (
                summary, claim, keywords, reliability_score,
                min(reliability_score, 95) if reliability_score else 80,
                datetime.now(), full_analysis_result, suspicious_points, update_summary_id
            ))
            
            conn.commit()
            logger.info(f"ë‰´ìŠ¤ {news_id} (Summary ID: {update_summary_id}) AI ë¶„ì„ ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ (íƒ€ì…: {analysis_type})")
            return True
            
        except Exception as e:
            logger.error(f"DB ì €ì¥ ì˜¤ë¥˜: {e}")
            if conn:
                conn.rollback()
            return False
        finally:
            if conn:
                cursor.close()
                conn.close()
    
    def notify_backend(self, news_id: int) -> bool:
        """ë°±ì—”ë“œì— ë¶„ì„ ì™„ë£Œ ì•Œë¦¼"""
        try:
            url = f"{self.backend_url}/api/news/{news_id}/analysis-complete"
            response = requests.post(url, timeout=10)
            
            if response.status_code == 200:
                logger.info(f"ë°±ì—”ë“œ ì•Œë¦¼ ì„±ê³µ: ë‰´ìŠ¤ {news_id}")
                return True
            else:
                logger.warning(f"ë°±ì—”ë“œ ì•Œë¦¼ ì‹¤íŒ¨: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"ë°±ì—”ë“œ ì•Œë¦¼ ì˜¤ë¥˜: {e}")
            return False
    
    def analyze_news_complete(self, news_id: int, title: str, content: str, source: str = "ì¶œì²˜ ë¶ˆëª…", analysis_type: str = "COMPREHENSIVE", prompt_type: str = None, summary_id: int = None) -> Dict:
        """ë‰´ìŠ¤ ì¢…í•© ë¶„ì„ ë° ê²°ê³¼ ì €ì¥"""
        try:
            logger.info(f"ë‰´ìŠ¤ {news_id} ì¢…í•© ë¶„ì„ ì‹œì‘")
            
            # AI ë¶„ì„ ìˆ˜í–‰ (prompt_type ìš°ì„  ì‚¬ìš©)
            effective_type = prompt_type or analysis_type
            analysis = self.analyze_news_content(title, content, source, effective_type)
            
            # DBì— ì €ì¥ (summary_idê°€ ì œê³µëœ ê²½ìš° ìš°ì„  ì‚¬ìš©)
            db_saved = self.save_analysis_to_db(news_id, analysis, analysis_type, summary_id)
            
            # ë°±ì—”ë“œ ì•Œë¦¼
            backend_notified = self.notify_backend(news_id)
            
            result = {
                "news_id": news_id,
                "analysis": analysis,
                "db_saved": db_saved,
                "backend_notified": backend_notified,
                "status": "completed" if db_saved else "failed"
            }
            
            logger.info(f"ë‰´ìŠ¤ {news_id} ë¶„ì„ ì™„ë£Œ - DB: {db_saved}, Backend: {backend_notified}")
            return result
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ {news_id} ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "news_id": news_id,
                "status": "failed",
                "error": str(e),
                "db_saved": False,
                "backend_notified": False
            }
