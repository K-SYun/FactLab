from fastapi import FastAPI, BackgroundTasks, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from app.crawlers.crawler_manager import UnifiedCrawlerManager
from app.api.crawler_api import router as crawler_router
# AI ë¶„ì„ì€ ê´€ë¦¬ìì—ì„œ ë³„ë„ ì²˜ë¦¬
from app.localization.messages import msg
import logging
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="FactLab Crawler AI Service", version="1.0.0")

# API ë¼ìš°í„° ë“±ë¡
app.include_router(crawler_router)

# CORS ì„¤ì • ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3001",  # Admin ì„œë¹„ìŠ¤
        "http://localhost:3000",  # User ì„œë¹„ìŠ¤
        "http://127.0.0.1:3001",
        "http://127.0.0.1:3000",
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™” - UnifiedCrawlerManager ì‚¬ìš©
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://factlab_user:password@localhost:5433/factlab")
crawler_manager = UnifiedCrawlerManager(DATABASE_URL)
# AI ë¶„ì„ê¸°ëŠ” ê´€ë¦¬ìì—ì„œ ë³„ë„ ì²˜ë¦¬

@app.on_event("startup")
async def startup_event():
    """ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
    logger.info("ğŸš€ Starting UnifiedCrawlerManager scheduler...")
    await crawler_manager.start_scheduler()
    logger.info("âœ… UnifiedCrawlerManager scheduler started successfully")

@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë¹„ìŠ¤ ì¢…ë£Œ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
    logger.info("ğŸ›‘ Stopping UnifiedCrawlerManager scheduler...")
    await crawler_manager.stop_scheduler()
    logger.info("âœ… UnifiedCrawlerManager scheduler stopped successfully")

@app.get("/")
def root():
    return {"message": "FactLab Crawler AI Service is running"}

@app.get("/health")
def health():
    return {"status": "healthy", "service": "crawler_ai"}

@app.post("/crawl/news")
async def crawl_news(background_tasks: BackgroundTasks, category: str = "politics"):
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ë° DB ì €ì¥"""
    async def crawl_task():
        try:
            result = await crawler_manager.manual_crawl_all()
            logger.info(f"Crawl result: {result}")
            return result
        except Exception as e:
            logger.error(f"Error crawling news: {e}")
            return {'saved': 0, 'duplicates': 0, 'errors': 1}
    
    background_tasks.add_task(crawl_task)
    return {"message": f"News crawling started"}

@app.post("/crawl/all")
async def crawl_all_news(background_tasks: BackgroundTasks):
    """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° DB ì €ì¥"""
    async def crawl_all_task():
        try:
            results = await crawler_manager.manual_crawl_all()
            logger.info(f"Full crawl results: {results}")
            return results
        except Exception as e:
            logger.error(f"Error crawling all news: {e}")
            return {}
    
    background_tasks.add_task(crawl_all_task)
    return {"message": "Full news crawling and saving started"}

@app.post("/crawl/daum")
async def crawl_daum_only(background_tasks: BackgroundTasks):
    """ë‹¤ìŒ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘ ë° DB ì €ì¥ (í…ŒìŠ¤íŠ¸ìš©)"""
    async def crawl_task():
        try:
            result = await crawler_manager.crawl_daum_news()
            logger.info(f"Daum crawl result: {result}")
            return result
        except Exception as e:
            logger.error(f"Error crawling Daum news: {e}")
            return {'saved': 0, 'duplicates': 0, 'errors': 1}
    
    background_tasks.add_task(crawl_task)
    return {"message": "Daum news crawling started"}

@app.post("/crawl/bills")
async def crawl_bills(background_tasks: BackgroundTasks, days: int = 30):
    """êµ­íšŒ ë²•ì•ˆ ìˆ˜ì§‘ ë° DB ì €ì¥"""
    async def crawl_task():
        try:
            result = await crawler_manager.crawl_bills(days=days)
            logger.info(f"Crawl result: {result}")
            return result
        except Exception as e:
            logger.error(f"Error crawling bills: {e}")
            return {'saved': 0, 'duplicates': 0, 'errors': 1}
    
    background_tasks.add_task(crawl_task)
    return {"message": f"Bill crawling started for the last {days} days"}

# AI ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸ 
@app.post("/analyze/news/{news_id}")
@app.post("/api/analyze/news/{news_id}")  # í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ ì¶”ê°€ ê²½ë¡œ
async def analyze_news(news_id: int):
    """ë‰´ìŠ¤ AI ë¶„ì„ ìˆ˜í–‰"""
    try:
        # ë‰´ìŠ¤ ì¡°íšŒ
        news = crawler_manager.db_manager.get_news_by_id(news_id)
        if not news:
            raise HTTPException(status_code=404, detail="ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # Gemini AI ë¶„ì„ê¸° ì‚¬ìš©
        from app.ai.gemini_analyzer import GeminiNewsAnalyzer
        ai_analyzer = GeminiNewsAnalyzer()
        
        # ì¢…í•© AI ë¶„ì„ ìˆ˜í–‰ (Gemini ê¸°ë°˜)
        analysis_result = ai_analyzer.analyze_news_complete(
            news_id=news_id,
            title=news.get('title', ''),
            content=news.get('content', '')
        )
        
        logger.info(f"AI analysis completed for news {news_id}")
        
        return {
            "success": True,
            "news_id": news_id,
            "title": news.get('title', ''),
            "analysis": analysis_result
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error analyzing news {news_id}: {e}")
        raise HTTPException(status_code=500, detail=f"AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.get("/db/test")
async def test_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        is_connected = crawler_manager.db_manager.test_connection()
        return {
            "status": "connected" if is_connected else "disconnected",
            "message": "Database connection successful" if is_connected else "Database connection failed"
        }
    except Exception as e:
        logger.error(f"Database test error: {e}")
        return {"status": "error", "message": str(e)}

@app.get("/db/stats")
async def get_database_stats():
    """ë°ì´í„°ë² ì´ìŠ¤ ë‰´ìŠ¤ í†µê³„"""
    try:
        stats = crawler_manager.db_manager.get_news_stats()
        return {"stats": stats}
    except Exception as e:
        logger.error(f"Error getting database stats: {e}")
        return {"error": str(e)}

@app.get("/news/pending")
async def get_pending_news(limit: int = 10):
    """ìŠ¹ì¸ ëŒ€ê¸° ì¤‘ì¸ ë‰´ìŠ¤ ì¡°íšŒ"""
    try:
        pending_news = crawler_manager.db_manager.get_pending_news(limit)
        return {"news": pending_news, "count": len(pending_news)}
    except Exception as e:
        logger.error(f"Error getting pending news: {e}")
        return {"error": str(e)}

@app.get("/categories")
async def get_categories():
    """ì§€ì›ë˜ëŠ” ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ"""
    try:
        categories = {
            "politics": msg.get("categories.politics"),
            "economy": msg.get("categories.economy"),
            "society": msg.get("categories.society"),
            "technology": msg.get("categories.technology"),
            "world": msg.get("categories.world"),
            "environment": msg.get("categories.environment")
        }
        return {"categories": categories}
    except Exception as e:
        logger.error(f"Error getting categories: {e}")
        return {"error": str(e)}

# ì¹´ë“œë‰´ìŠ¤ ìƒì„± ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/card-news/generate")
async def generate_card_news(request_data: dict):
    """AI ë¶„ì„ ì™„ë£Œëœ ë‰´ìŠ¤ë¡œ ì¹´ë“œë‰´ìŠ¤ ìƒì„±"""
    try:
        from app.ai.card_news_generator import CardNewsGenerator

        news_data = request_data.get("news", {})
        num_slides = request_data.get("num_slides", 5)

        # í•„ìˆ˜ ë°ì´í„° ê²€ì¦
        if not news_data:
            raise HTTPException(status_code=400, detail="ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        news_id = news_data.get("newsId")
        title = news_data.get("newsTitle", "")
        content = news_data.get("newsContent", "")
        summary = news_data.get("aiSummary", "")
        claim = news_data.get("aiClaim", "")
        keywords = news_data.get("aiKeywords", "")
        thumbnail_url = news_data.get("newsThumbnailUrl", "")

        if not news_id or not title:
            raise HTTPException(status_code=400, detail="í•„ìˆ˜ ë°ì´í„°ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ì¹´ë“œë‰´ìŠ¤ ìƒì„±ê¸° ì´ˆê¸°í™”
        generator = CardNewsGenerator()

        # ì¹´ë“œë‰´ìŠ¤ ìƒì„±
        card_news = generator.generate_card_news_complete(
            news_id=news_id,
            title=title,
            content=content,
            summary=summary,
            claim=claim,
            keywords=keywords,
            thumbnail_url=thumbnail_url,
            num_slides=num_slides
        )

        logger.info(f"Card news generated successfully for news {news_id}")

        return {
            "success": True,
            "message": "ì¹´ë“œë‰´ìŠ¤ ìƒì„± ì„±ê³µ",
            "card_news": card_news
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating card news: {e}")
        raise HTTPException(status_code=500, detail=f"ì¹´ë“œë‰´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
