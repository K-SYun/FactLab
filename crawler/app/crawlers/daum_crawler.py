"""
ë‹¤ìŒ ëª¨ë°”ì¼ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
- ë‹¤ìŒ ëª¨ë°”ì¼ ì‚¬ì´íŠ¸ì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
- 2ì‹œê°„ ê°„ê²©, 5ì´ˆë‹¹ 1ê°œì”© 20ê°œ í¬ë¡¤ë§
"""

import asyncio
import aiohttp
import logging
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict
import re
import hashlib
from urllib.parse import urljoin, quote
import random
from dataclasses import dataclass
import sys
import os
from pathlib import Path
from PIL import Image
import io
import base64

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from app.database.db_manager import DatabaseManager

logger = logging.getLogger(__name__)

@dataclass
class NewsItem:
    title: str
    content: str
    url: str
    source: str
    category: str
    publish_date: datetime
    thumbnail: str = None
    url_hash: str = None
    
    def __post_init__(self):
        if self.url_hash is None:
            self.url_hash = hashlib.md5(self.url.encode()).hexdigest()

class DaumMobileCrawler:
    def __init__(self):
        self.base_url = "https://m.media.daum.net"
        self.session = None
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.db_manager = DatabaseManager()
        
        # ë‹¤ìŒ ëª¨ë°”ì¼ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        self.categories = {
            "politics": {"code": "politics", "name": "ì •ì¹˜"},
            "economy": {"code": "economic", "name": "ê²½ì œ"},
            "society": {"code": "society", "name": "ì‚¬íšŒ"},
            "technology": {"code": "digital", "name": "IT/ê³¼í•™"},
            "world": {"code": "foreign", "name": "ì„¸ê³„"},
            "environment": {"code": "climate", "name": "ê¸°í›„/í™˜ê²½"}
        }
        
        # ëª¨ë°”ì¼ User-Agent
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://m.media.daum.net/'
        }
        
    async def init_session(self):
        """HTTP ì„¸ì…˜ ì´ˆê¸°í™”"""
        if not self.session:
            timeout = aiohttp.ClientTimeout(total=30, connect=10)
            connector = aiohttp.TCPConnector(
                limit=10, 
                limit_per_host=5,
                ssl=False  # SSL ê²€ì¦ ë¹„í™œì„±í™”
            )
            self.session = aiohttp.ClientSession(
                headers=self.headers,
                timeout=timeout,
                connector=connector
            )
            logger.info("Daum mobile crawler session initialized")
    
    async def close_session(self):
        """HTTP ì„¸ì…˜ ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
            self.session = None
            logger.info("Daum mobile crawler session closed")
    
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = re.sub(r'[\r\n\t]', ' ', text)
        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£.,!?()[\]\-]', '', text)
        
        return text.strip()
    
    def _extract_video_thumbnail(self, soup: BeautifulSoup) -> str:
        """ë™ì˜ìƒ ì¸ë„¤ì¼ ì¶”ì¶œ"""
        try:
            # ìš°ì„ ìˆœìœ„ë³„ ë™ì˜ìƒ ì¸ë„¤ì¼ ì¶”ì¶œ ë°©ë²•
            
            # 1ìˆœìœ„: video íƒœê·¸ì˜ poster ì†ì„±
            video_tags = soup.find_all('video')
            for video in video_tags:
                poster = video.get('poster')
                if poster:
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if poster.startswith('/'):
                        poster = f"https://media.daum.net{poster}"
                    elif not poster.startswith('http'):
                        poster = f"https://{poster}"
                    logger.debug(f"video poster ë°œê²¬: {poster}")
                    return poster
            
            # 2ìˆœìœ„: ë‹¤ìŒ ë™ì˜ìƒ íŠ¹í™” íŒ¨í„´
            daum_video_selectors = [
                # ë‹¤ìŒ ë™ì˜ìƒ ì¸ë„¤ì¼ í´ë˜ìŠ¤
                'img[class*="video"]',
                'img[class*="thumbnail"]',
                '.video_thumb img',
                '.video_thumbnail img',
                '.article_video img',
                'img[src*="video"]',
                'img[src*="thumb"]'
            ]
            
            for selector in daum_video_selectors:
                img_tags = soup.select(selector)
                for img_tag in img_tags:
                    src = img_tag.get('src') or img_tag.get('data-src')
                    if src:
                        # ë™ì˜ìƒ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                        if any(keyword in src.lower() for keyword in ['video', 'thumb', 'poster']):
                            logger.debug(f"ë™ì˜ìƒ ê´€ë ¨ ì´ë¯¸ì§€ ë°œê²¬: {src}")
                            return src
            
            # 3ìˆœìœ„: og:video:thumbnail ë©”íƒ€íƒœê·¸
            meta_video_thumbnail = soup.find('meta', property='og:video:thumbnail')
            if meta_video_thumbnail and meta_video_thumbnail.get('content'):
                logger.debug(f"og:video:thumbnail ë°œê²¬: {meta_video_thumbnail['content']}")
                return meta_video_thumbnail['content']
            
            # 4ìˆœìœ„: ìœ íŠœë¸Œ ë“± ì™¸ë¶€ ë™ì˜ìƒ iframeì—ì„œ ì¸ë„¤ì¼ ì¶”ì¶œ
            iframe_tags = soup.find_all('iframe')
            for iframe in iframe_tags:
                src = iframe.get('src', '')
                if 'youtube.com' in src or 'youtu.be' in src:
                    # ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ
                    video_id = None
                    if 'embed/' in src:
                        video_id = src.split('embed/')[-1].split('?')[0]
                    elif 'v=' in src:
                        video_id = src.split('v=')[-1].split('&')[0]
                    
                    if video_id:
                        youtube_thumbnail = f"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg"
                        logger.debug(f"ìœ íŠœë¸Œ ì¸ë„¤ì¼ ìƒì„±: {youtube_thumbnail}")
                        return youtube_thumbnail
                        
                elif 'vimeo.com' in src:
                    # Vimeo ë¹„ë””ì˜¤ ID ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
                    video_id = src.split('/')[-1].split('?')[0]
                    if video_id.isdigit():
                        vimeo_thumbnail = f"https://vumbnail.com/{video_id}.jpg"
                        logger.debug(f"Vimeo ì¸ë„¤ì¼ ìƒì„±: {vimeo_thumbnail}")
                        return vimeo_thumbnail
                        
            return None
            
        except Exception as e:
            logger.error(f"ë™ì˜ìƒ ì¸ë„¤ì¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    

    def extract_thumbnail(self, soup: BeautifulSoup) -> str:
        """ì¸ë„¤ì¼ ì´ë¯¸ì§€ URL ì¶”ì¶œ (ë™ì˜ìƒ ì¸ë„¤ì¼ ìš°ì„ , ë³¸ë¬¸ ì´ë¯¸ì§€ í¬í•¨, ì–¸ë¡ ì‚¬ ë¡œê³  ì œì™¸)"""
        try:
            # ì–¸ë¡ ì‚¬ ë¡œê³ ë‚˜ ë¶€ì ì ˆí•œ ì´ë¯¸ì§€ë¥¼ í•„í„°ë§í•  í‚¤ì›Œë“œ (ê°•í™”)
            # 'thumb' ì œê±°: ë‹¤ìŒ CDNì—ì„œ thumbì€ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• ì„œë¹„ìŠ¤ëª…ì´ë¯€ë¡œ í•„í„°ë§í•˜ë©´ ì•ˆë¨
            skip_keywords_url = [
                'logo', 'icon', 'button', 'banner', 'ad', 'advertisement', 
                'profile', 'avatar', 'favicon', 'symbol', 'emblem', 'mark', 'ci',
                'header', 'footer', 'nav', 'menu', 'bg', 'background'
            ]
            skip_keywords_alt = [
                # í•œêµ­ ì–¸ë¡ ì‚¬
                'ë¡œê³ ', 'logo', 'ì•„ì´ì½˜', 'icon', 'ê´‘ê³ ', 'ad', 'advertisement', 
                'ë°°ë„ˆ', 'banner', 'ë¸Œëœë“œ', 'brand', 'ì–¸ë¡ ì‚¬', 'news', 'ë‰´ìŠ¤',
                'ë°©ì†¡êµ­', 'tv', 'ë¼ë””ì˜¤', 'radio', 'ê¸°ì', 'reporter', 'ì•„ë‚˜ìš´ì„œ',
                'sbs', 'kbs', 'mbc', 'jtbc', 'ytn', 'tvn', 'yna', 'ì—°í•©ë‰´ìŠ¤',
                'ì¡°ì„ ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 'ë™ì•„ì¼ë³´', 'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸', 'í•œêµ­ì¼ë³´',
                'ë§¤ì¼ê²½ì œ', 'í•œêµ­ê²½ì œ', 'ì„œìš¸ì‹ ë¬¸', 'êµ­ë¯¼ì¼ë³´', 'ë¬¸í™”ì¼ë³´', 'ì„¸ê³„ì¼ë³´',
                # ë‹¤ìŒ íŠ¹í™”
                'ë‹¤ìŒ', 'daum', 'kakao', 'ì¹´ì¹´ì˜¤',
                # ì¼ë°˜ì ì¸ UI ìš”ì†Œ
                'í”„ë¡œí•„', 'profile', 'ì¸ë„¤ì¼', 'thumbnail', 'ë¯¸ë¦¬ë³´ê¸°', 'preview',
                'íšŒì‚¬', 'company', 'ê¸°ì—…', 'corp', 'ë‹¨ì²´', 'organization'
            ]
            
            # 1ë‹¨ê³„: ë™ì˜ìƒ ì¸ë„¤ì¼ ì¶”ì¶œ ì‹œë„
            video_thumbnail = self._extract_video_thumbnail(soup)
            if video_thumbnail and self._is_valid_thumbnail(video_thumbnail, '', skip_keywords_url, skip_keywords_alt):
                logger.debug(f"ë™ì˜ìƒ ì¸ë„¤ì¼ ì¶”ì¶œ: {video_thumbnail}")
                return video_thumbnail
            
            # ìš°ì„ ìˆœìœ„ë³„ ì¸ë„¤ì¼ ì¶”ì¶œ íŒ¨í„´ (ë³¸ë¬¸ ì´ë¯¸ì§€ ìš°ì„ )
            priority_selectors = [
                # 1ìˆœìœ„: ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ (ê°€ì¥ ì¤‘ìš”)
                '.news_view .article_view img:not([class*="logo"]):not([class*="icon"])',
                '.article_view img:not([class*="logo"]):not([class*="icon"])',
                '.news_content img:not([class*="logo"]):not([class*="icon"])',
                '.article_body img:not([class*="logo"]):not([class*="icon"])',
                
                # 2ìˆœìœ„: ë‹¤ìŒ ë‰´ìŠ¤ ë©”ì¸ ì´ë¯¸ì§€
                '.news_view img',
                '.article_view img',
                
                # 3ìˆœìœ„: ë©”íƒ€íƒœê·¸ ì´ë¯¸ì§€ (ìµœí›„ ìˆ˜ë‹¨)
                'meta[property="og:image"]',
                'meta[name="twitter:image"]',
                'meta[property="image"]'
            ]
            
            # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰
            for selector in priority_selectors:
                if selector.startswith('meta'):
                    meta_tag = soup.select_one(selector)
                    if meta_tag and meta_tag.get('content'):
                        img_url = meta_tag['content']
                        if self._is_valid_thumbnail(img_url, '', skip_keywords_url, skip_keywords_alt):
                            logger.debug(f"ë©”íƒ€íƒœê·¸ì—ì„œ ì¸ë„¤ì¼ ì¶”ì¶œ: {img_url}")
                            return img_url
                else:
                    img_tags = soup.select(selector)
                    for img_tag in img_tags:
                        src = img_tag.get('src') or img_tag.get('data-src')
                        alt = img_tag.get('alt', '')
                        if src and self._is_valid_thumbnail(src, alt, skip_keywords_url, skip_keywords_alt):
                            logger.debug(f"DOMì—ì„œ ì¸ë„¤ì¼ ì¶”ì¶œ: {src} (alt: {alt})")
                            return src
            
            # ëŒ€ì•ˆ: ë³¸ë¬¸ì—ì„œ í¬ê¸°ê°€ ì¶©ë¶„í•œ ì´ë¯¸ì§€ ì°¾ê¸°
            article_areas = [
                '.news_view', '.article_view', '.news_content', '.article_body'
            ]
            
            for area_selector in article_areas:
                area = soup.select_one(area_selector)
                if area:
                    images = area.find_all('img')
                    for img in images:
                        src = img.get('src') or img.get('data-src')
                        alt = img.get('alt', '')
                        width = img.get('width', '')
                        height = img.get('height', '')
                        
                        if src and self._is_valid_thumbnail(src, alt, skip_keywords_url, skip_keywords_alt):
                            # í¬ê¸° ì •ë³´ê°€ ìˆìœ¼ë©´ í™•ì¸ (ìµœì†Œ 100px)
                            if width and height:
                                try:
                                    if int(width) >= 100 and int(height) >= 100:
                                        logger.debug(f"ë³¸ë¬¸ì—ì„œ í¬ê¸° ê¸°ë°˜ ì¸ë„¤ì¼ ì¶”ì¶œ: {src}")
                                        return src
                                except:
                                    pass
                            else:
                                logger.debug(f"ë³¸ë¬¸ì—ì„œ ì¸ë„¤ì¼ ì¶”ì¶œ: {src}")
                                return src
                                
            logger.debug("ì ì ˆí•œ ì¸ë„¤ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
            
        except Exception as e:
            logger.error(f"ì¸ë„¤ì¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _is_valid_thumbnail(self, src: str, alt: str, skip_keywords_url: list, skip_keywords_alt: list) -> bool:
        """ì¸ë„¤ì¼ì´ ì ì ˆí•œì§€ ê²€ì‚¬ (URLê³¼ alt ê°’ ê¸°ì¤€) - ê°•í™”ëœ í•„í„°ë§"""
        try:
            # URL ìœ íš¨ì„± ê²€ì‚¬
            if not src or not src.startswith(('http://', 'https://', 'data:')):
                return False
            
            src_lower = src.lower()
            alt_lower = alt.lower()
            
            # URLì— ë¶€ì ì ˆí•œ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê²€ì‚¬ (ê°•í™”)
            for keyword in skip_keywords_url:
                if keyword in src_lower:
                    logger.debug(f"URL í‚¤ì›Œë“œ í•„í„°ë§: {keyword} in {src}")
                    return False
            
            # alt ê°’ì— ë¶€ì ì ˆí•œ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê²€ì‚¬ (ê°•í™”)  
            for keyword in skip_keywords_alt:
                if keyword in alt_lower:
                    logger.debug(f"ALT í‚¤ì›Œë“œ í•„í„°ë§: {keyword} in {alt}")
                    return False
            
            # íŒŒì¼ í™•ì¥ì ê²€ì‚¬ (ì´ë¯¸ì§€ íŒŒì¼ë§Œ)
            if not any(ext in src_lower for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                # data: URLì˜ ê²½ìš°ëŠ” í—ˆìš©
                if not src.startswith('data:image/'):
                    logger.debug(f"ì´ë¯¸ì§€ íŒŒì¼ í™•ì¥ìê°€ ì•„ë‹˜: {src}")
                    return False
            
            # ì´ë¯¸ì§€ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ì€ ê²ƒ ì œì™¸ (URLì—ì„œ í¬ê¸° ì •ë³´ í™•ì¸)
            # ë‹¤ìŒ CDNì˜ thumbì€ ì„¬ë„¤ì¼ ì„œë¹„ìŠ¤ì´ë¯€ë¡œ ì œì™¸í•˜ê³ , ì‹¤ì œ ì‘ì€ í¬ê¸°ë§Œ í•„í„°ë§
            small_sizes = [
                '16x16', '24x24', '32x32', '48x48', '64x64', '80x80',
                '16_16', '24_24', '32_32', '48_48', '64_64', '80_80',
                'small', 'mini', 'tiny'
            ]
            # daumcdn.netì˜ thumb ì„œë¹„ìŠ¤ëŠ” ì œì™¸ (ì‹¤ì œë¡œëŠ” í° ì´ë¯¸ì§€ì„)
            if not 'daumcdn.net/thumb' in src_lower:
                if any(size in src_lower for size in small_sizes):
                    logger.debug(f"ì´ë¯¸ì§€ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {src}")
                    return False
            
            # ë‹¤ìŒ íŠ¹ì • ë¡œê³ /ì•„ì´ì½˜ íŒ¨í„´ ì œì™¸
            daum_logo_patterns = [
                't1.daumcdn.net/news/logo', 'img.daumcdn.net/logo',
                'daumcdn.net/static', 'kakao.com/logo', 'daum.net/favicon'
            ]
            if any(pattern in src_lower for pattern in daum_logo_patterns):
                logger.debug(f"ë‹¤ìŒ ë¡œê³  íŒ¨í„´ í•„í„°ë§: {src}")
                return False
            
            # ìµœì†Œ í’ˆì§ˆ ê¸°ì¤€: URLì— resolutionì´ë‚˜ width ì •ë³´ê°€ ìˆëŠ” ê²½ìš° í™•ì¸
            resolution_patterns = ['w=', 'width=', 'size=', 'res=']
            for pattern in resolution_patterns:
                if pattern in src_lower:
                    try:
                        # w=100 ê°™ì€ íŒ¨í„´ì—ì„œ í¬ê¸° ì¶”ì¶œ
                        import re
                        match = re.search(f'{pattern}(\\d+)', src_lower)
                        if match:
                            size = int(match.group(1))
                            if size < 120:  # ìµœì†Œ 120px
                                logger.debug(f"URLì—ì„œ ì¶”ì¶œí•œ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {size}px")
                                return False
                    except:
                        pass
                        
            return True
            
        except Exception as e:
            logger.debug(f"ì¸ë„¤ì¼ ìœ íš¨ì„± ê²€ì‚¬ ì˜¤ë¥˜: {e}")
            return False
    
    def extract_news_source(self, soup: BeautifulSoup) -> str:
        """ë‹¤ìŒ ë‰´ìŠ¤ì—ì„œ ì‹¤ì œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ"""
        try:
            # 1ìˆœìœ„: ë‹¤ìŒ ë‰´ìŠ¤ êµ¬ì¡°ì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ
            source_selectors = [
                '.info_news .txt_info',  # ë‹¤ìŒ ë‰´ìŠ¤ ì •ë³´ ì˜ì—­
                '.head_view .info_news .link_cp',  # ì–¸ë¡ ì‚¬ ë§í¬
                '.info_news .txt_cp',  # ì–¸ë¡ ì‚¬ëª…
                '.news_cp img',  # ì–¸ë¡ ì‚¬ ë¡œê³  ì´ë¯¸ì§€ì˜ alt
                '.cp_name',  # ì–¸ë¡ ì‚¬ëª… í´ë˜ìŠ¤
                '.source',  # ì¶œì²˜
                '.media_cp_name',  # ë¯¸ë””ì–´ ì–¸ë¡ ì‚¬ëª…
            ]
            
            for selector in source_selectors:
                elements = soup.select(selector)
                for element in elements:
                    # img íƒœê·¸ì˜ alt ì†ì„±ì—ì„œ ì¶”ì¶œ
                    if element.name == 'img':
                        alt_text = element.get('alt', '').strip()
                        if alt_text and alt_text not in ['logo', 'icon', 'ë¡œê³ ']:
                            logger.info(f"âœ… ë‹¤ìŒ ì´ë¯¸ì§€ altì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ: {alt_text}")
                            return alt_text
                    
                    # í…ìŠ¤íŠ¸ ìš”ì†Œì—ì„œ ì¶”ì¶œ
                    text = element.get_text().strip()
                    if text and len(text) > 1 and len(text) < 20:
                        logger.info(f"âœ… ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ: {text}")
                        return text
                        
                    # ë§í¬ì˜ title ì†ì„±ì—ì„œ ì¶”ì¶œ
                    title_text = element.get('title', '').strip()
                    if title_text and 'ì–¸ë¡ ì‚¬' not in title_text and len(title_text) < 20:
                        logger.info(f"âœ… ë‹¤ìŒ title ì†ì„±ì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ: {title_text}")
                        return title_text
            
            # 2ìˆœìœ„: meta íƒœê·¸ì—ì„œ ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
            # og:article:author ë¨¼ì € í™•ì¸
            meta_og_author = soup.find('meta', property='og:article:author')
            if meta_og_author and meta_og_author.get('content'):
                source = meta_og_author['content'].strip()
                if source and len(source) < 30:  # ê¸¸ì´ ì œí•œ ì™„í™”
                    logger.info(f"âœ… ë‹¤ìŒ meta og:article:authorì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ: {source}")
                    return source
            
            # og:site_nameì—ì„œ ì¶”ì¶œ (ë‹¤ìŒ - ì–¸ë¡ ì‚¬ëª… í˜•íƒœ)
            meta_og_site_name = soup.find('meta', property='og:site_name')
            if meta_og_site_name and meta_og_site_name.get('content'):
                site_name = meta_og_site_name['content'].strip()
                if site_name and 'ë‹¤ìŒ -' in site_name:
                    # "ë‹¤ìŒ - í—¤ëŸ´ë“œê²½ì œ" í˜•íƒœì—ì„œ ì–¸ë¡ ì‚¬ëª…ë§Œ ì¶”ì¶œ
                    source = site_name.split('ë‹¤ìŒ -')[-1].strip()
                    if source and len(source) < 30:
                        logger.info(f"âœ… ë‹¤ìŒ meta og:site_nameì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ: {source}")
                        return source
                        
            # ê¸°ì¡´ article:authorë„ í™•ì¸ (í˜¸í™˜ì„±)
            meta_source = soup.find('meta', property='article:author')
            if meta_source and meta_source.get('content'):
                source = meta_source['content'].strip()
                if source and len(source) < 30:
                    logger.info(f"âœ… ë‹¤ìŒ meta article:authorì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ: {source}")
                    return source
            
            # 3ìˆœìœ„: JSON-LDì—ì„œ publisher ì •ë³´ ì¶”ì¶œ
            script_tags = soup.find_all('script', type='application/ld+json')
            for script in script_tags:
                try:
                    import json
                    data = json.loads(script.get_text())
                    if isinstance(data, dict):
                        publisher = data.get('publisher', {})
                        if isinstance(publisher, dict) and 'name' in publisher:
                            publisher_name = publisher['name'].strip()
                            if publisher_name and len(publisher_name) < 20:
                                logger.info(f"âœ… ë‹¤ìŒ JSON-LDì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ: {publisher_name}")
                                return publisher_name
                except Exception as e:
                    logger.debug(f"ë‹¤ìŒ JSON-LD íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            logger.warning("âš ï¸ ë‹¤ìŒ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ ì‹¤íŒ¨, ì•Œìˆ˜ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬")
            return "ì•Œìˆ˜ì—†ìŒ"
            
        except Exception as e:
            logger.error(f"âŒ ë‹¤ìŒ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return "ì•Œìˆ˜ì—†ìŒ"
    
    async def resize_and_optimize_image(self, image_url: str, 
                                      mobile_size: tuple = (200, 150), 
                                      desktop_size: tuple = (400, 300), 
                                      retina_size: tuple = (800, 600)) -> str:
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ, ë¦¬ì‚¬ì´ì¦ˆ í›„ íŒŒì¼ë¡œ ì €ì¥ - íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        try:
            if not image_url:
                return None
                
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            headers = {
                'User-Agent': self.user_agent,
                'Referer': 'https://news.daum.net/'
            }
            
            async with self.session.get(image_url, headers=headers, timeout=10) as response:
                if response.status != 200:
                    logger.debug(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({image_url}): HTTP {response.status}")
                    return image_url
                    
                image_data = await response.read()
                
                # ì´ë¯¸ì§€ í¬ê¸° ì²´í¬ (10MB ì œí•œ)
                if len(image_data) > 10 * 1024 * 1024:
                    logger.debug(f"ì´ë¯¸ì§€ í¬ê¸° ì´ˆê³¼ ({image_url}): {len(image_data)} bytes")
                    return image_url
                
            # PILë¡œ ì´ë¯¸ì§€ ì²˜ë¦¬
            image = Image.open(io.BytesIO(image_data))
            
            # RGBA ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜ (JPEG ì €ì¥ì„ ìœ„í•¨)
            if image.mode in ('RGBA', 'LA', 'P'):
                # í°ìƒ‰ ë°°ê²½ìœ¼ë¡œ ë³€í™˜
                background = Image.new('RGB', image.size, (255, 255, 255))
                if image.mode == 'P':
                    image = image.convert('RGBA')
                background.paste(image, mask=image.split()[-1] if image.mode == 'RGBA' else None)
                image = background
            
            # ë°ìŠ¤í¬íƒ‘ìš© ì¸ë„¤ì¼ ìƒì„± (400x300, ê³ í•´ìƒë„ëŠ” 800x600)
            image_desktop = image.copy()
            image_desktop.thumbnail(desktop_size, Image.Resampling.LANCZOS)
            
            # ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„± (URL í•´ì‹œ + íƒ€ì„ìŠ¤íƒ¬í”„)
            import hashlib
            url_hash = hashlib.md5(image_url.encode()).hexdigest()[:8]
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"thumb_daum_{url_hash}_{timestamp}.jpg"
            
            # ì¸ë„¤ì¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            thumb_dir = "/app/thumbnails"
            os.makedirs(thumb_dir, exist_ok=True)
            
            # ë°ìŠ¤í¬íƒ‘ìš© ì¸ë„¤ì¼ ì €ì¥
            desktop_path = os.path.join(thumb_dir, filename)
            image_desktop.save(desktop_path, format='JPEG', quality=85, optimize=True)
            
            # ì›¹ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ ê²½ë¡œ ë°˜í™˜ (nginxë¥¼ í†µí•´ ì„œë¹™)
            web_path = f"/thumbnails/{filename}"
            logger.debug(f"ë‹¤ìŒ ì¸ë„¤ì¼ ì €ì¥ ì™„ë£Œ: {web_path} (í¬ê¸°: {image_desktop.size})")
            
            return web_path
            
        except Exception as e:
            logger.debug(f"ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨ ({image_url}): {e}")
            return image_url  # ì›ë³¸ URL ë°˜í™˜
    
    async def get_category_news_list(self, category: str, count: int = 20) -> List[str]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ëª©ë¡ URL ìˆ˜ì§‘"""
        if category not in self.categories:
            logger.error(f"Invalid category: {category}")
            return []
        
        category_info = self.categories[category]
        # ë‹¤ìŒ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ URL (ëª¨ë°”ì¼ì´ ì•„ë‹Œ ì¼ë°˜ í˜ì´ì§€ ì‚¬ìš©)
        list_url = f"https://news.daum.net/{category_info['code']}"
        
        try:
            await asyncio.sleep(random.uniform(1, 3))  # ëœë¤ ì§€ì—°
            
            async with self.session.get(list_url) as response:
                if response.status != 200:
                    logger.error(f"Failed to fetch news list: {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # ë‰´ìŠ¤ ë§í¬ ì¶”ì¶œ
                news_links = []
                
                # ë‹¤ìŒ ë‰´ìŠ¤ ë§í¬ íŒ¨í„´ (v.daum.net í˜•íƒœ)
                articles = soup.find_all('a', href=re.compile(r'v\.daum\.net/v/')) or \
                          soup.find_all('a', href=re.compile(r'/v/\d+')) or \
                          soup.find_all('div', class_='item_mainnews') or \
                          soup.find_all('strong', class_='tit_thumb') or \
                          soup.find_all('a')
                
                for article in articles[:count * 3]:  # ì—¬ìœ ìˆê²Œ ìˆ˜ì§‘
                    link_elem = article if article.name == 'a' else article.find('a')
                    
                    if link_elem and link_elem.get('href'):
                        href = link_elem['href']
                        
                        # ë‹¤ìŒ ë‰´ìŠ¤ URL ì •ê·œí™”
                        if href.startswith('/v/'):
                            # /v/20250815105429103 í˜•íƒœë¥¼ https://v.daum.net/v/20250815105429103ë¡œ ë³€í™˜
                            full_url = f"https://v.daum.net{href}"
                        elif 'v.daum.net/v/' in href:
                            # ì´ë¯¸ ì™„ì „í•œ URL
                            full_url = href
                        elif href.startswith('http') and 'v.daum.net' in href:
                            full_url = href
                        else:
                            continue
                        
                        # ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ì¸ì§€ í™•ì¸ (v.daum.net/v/ íŒ¨í„´)
                        if ('v.daum.net/v/' in full_url and 
                            full_url not in news_links and 
                            len(full_url.split('/v/')[-1]) >= 10):  # IDê°€ ì¶©ë¶„íˆ ê¸´ì§€ í™•ì¸
                            news_links.append(full_url)
                
                logger.info(f"Found {len(news_links)} news links for category {category}")
                return news_links[:count]
                
        except Exception as e:
            logger.error(f"Error fetching news list for {category}: {e}")
            return []
    
    async def crawl_news_article(self, url: str, category: str) -> NewsItem:
        """ê°œë³„ ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§"""
        try:
            await asyncio.sleep(5)  # 5ì´ˆ ê°„ê²©
            
            async with self.session.get(url) as response:
                if response.status != 200:
                    logger.error(f"Failed to fetch article: {response.status}")
                    return None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # ì œëª© ì¶”ì¶œ - ë‹¤ìŒ ë‰´ìŠ¤ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                title_elem = soup.find('h1', class_='tit_view') or \
                           soup.find('h3', class_='tit_view') or \
                           soup.find('h1', class_='news_title') or \
                           soup.find('h2', class_='news_title') or \
                           soup.find('h1') or \
                           soup.find('h2')
                
                title = self.clean_text(title_elem.get_text()) if title_elem else "ì œëª© ì—†ìŒ"
                
                # ë‹¤ìŒ ë‰´ìŠ¤ íŠ¹ìœ ì˜ ë¶ˆí•„ìš”í•œ ì ‘ë‘ì‚¬ ì œê±°
                if title:
                    # "[ìˆ«ì] ì†ë³´" í˜•íƒœ ì œê±°
                    title = re.sub(r'^\[\d+\]\s*ì†ë³´\s*', '', title)
                    # "[ì¹´í…Œê³ ë¦¬]" í˜•íƒœ ì ‘ë‘ì‚¬ ì œê±° (ì˜ˆ: [ì •ì¹˜], [ê²½ì œ] ë“±)
                    title = re.sub(r'^\[[^\]]+\]\s*', '', title)
                    # "ì†ë³´:" ë˜ëŠ” "ë‹¨ë…:" ë“± ì ‘ë‘ì‚¬ ì œê±°
                    title = re.sub(r'^(ì†ë³´|ë‹¨ë…|ê¸´ê¸‰|íŠ¹ë³´)\s*[:ï¼š]\s*', '', title)
                    title = title.strip()
                
                # ë³¸ë¬¸ ì¶”ì¶œ - ë‹¤ìŒ ë‰´ìŠ¤ êµ¬ì¡°
                content_elem = soup.find('div', class_='news_view') or \
                              soup.find('div', id='harmonyContainer') or \
                              soup.find('section', class_='news_view') or \
                              soup.find('div', class_='article_view')
                
                content = ""
                if content_elem:
                    # ìŠ¤í¬ë¦½íŠ¸, ê´‘ê³  ë“± ì œê±°
                    for unwanted in content_elem.find_all(['script', 'style', 'iframe', 'ins', 'figure']):
                        unwanted.decompose()
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    paragraphs = content_elem.find_all(['p', 'div'])
                    content_parts = []
                    
                    for p in paragraphs:
                        text = self.clean_text(p.get_text())
                        if text and len(text) > 10:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                            content_parts.append(text)
                    
                    content = ' '.join(content_parts)
                
                if not content:
                    content = "ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                # ê¸°ì‚¬ ê¸¸ì´ ì œí•œ
                if len(content) > 2000:
                    content = content[:2000] + "..."
                
                # ì¸ë„¤ì¼ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ë¦¬ì‚¬ì´ì¦ˆ
                thumbnail_url = self.extract_thumbnail(soup)
                if thumbnail_url:
                    thumbnail_url = await self.resize_and_optimize_image(thumbnail_url)
                
                # ì‹¤ì œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ
                actual_source = self.extract_news_source(soup)
                
                # ìˆ˜ì§‘ëœ ì¹´í…Œê³ ë¦¬ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í˜ì´ì§€ì—ì„œ ìˆ˜ì§‘ë˜ì—ˆìœ¼ë¯€ë¡œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜)
                final_category = category
                
                news_item = NewsItem(
                    title=title,
                    content=content,
                    url=url,
                    source=actual_source,  # ì‹¤ì œ ì–¸ë¡ ì‚¬ëª… ì‚¬ìš©
                    category=final_category,
                    publish_date=datetime.now(),
                    thumbnail=thumbnail_url
                )
                
                logger.info(f"Successfully crawled: {title[:50]}...")
                return news_item
                
        except Exception as e:
            logger.error(f"Error crawling article {url}: {e}")
            return None

    async def save_news_to_db(self, news_item: NewsItem) -> bool:
        """ë‰´ìŠ¤ 1ê±´ì„ DBì— ì €ì¥"""
        try:
            # NewsItemì— source_urlì´ ì—†ìœ¼ë©´ url ê°’ìœ¼ë¡œ ì„¤ì •
            if not hasattr(news_item, 'source_url'):
                setattr(news_item, 'source_url', news_item.url)

            news_id = self.db_manager.save_news_item(news_item)
            if news_id:
                return True
            else:
                return False

        except Exception as e:
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def crawl_category(self, category: str, count: int = 20) -> List[NewsItem]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        logger.info(f"Starting crawl for category: {category} (target: {count} articles)")
        
        await self.init_session()
        
        try:
            # 1. ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘
            news_urls = await self.get_category_news_list(category, count)
            
            if not news_urls:
                logger.warning(f"No news URLs found for category: {category}")
                return []
            
            # 2. ê°œë³„ ê¸°ì‚¬ í¬ë¡¤ë§ (5ì´ˆ ê°„ê²©) - ì¦‰ì‹œ DB ì €ì¥
            news_items = []
            saved_count = 0
            for i, url in enumerate(news_urls):
                logger.info(f"Crawling article {i+1}/{len(news_urls)}: {url}")

                news_item = await self.crawl_news_article(url, category)
                if news_item:
                    news_items.append(news_item)

                    # ì¦‰ì‹œ DB ì €ì¥
                    if await self.save_news_to_db(news_item):
                        saved_count += 1
                        logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: {news_item.title[:50]}")

                # ì§„í–‰ë¥  ë¡œê·¸
                if (i + 1) % 5 == 0:
                    logger.info(f"Progress: {i+1}/{len(news_urls)} articles crawled, {saved_count} saved")
            
            logger.info(f"Completed crawling for {category}: {len(news_items)} articles collected")
            return news_items
            
        except Exception as e:
            logger.error(f"Error in crawl_category for {category}: {e}")
            return []
        
        finally:
            await self.close_session()
    
    async def crawl_all_categories(self, count_per_category: int = 20) -> Dict[str, List[NewsItem]]:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        logger.info(f"Starting crawl for all Daum categories (target: {count_per_category} per category)")
        
        results = {}
        
        for category in self.categories.keys():
            logger.info(f"Starting crawl for category: {category}")
            try:
                news_items = await self.crawl_category(category, count_per_category)
                results[category] = news_items
                
                # ì¹´í…Œê³ ë¦¬ ê°„ ê°„ê²©
                if category != list(self.categories.keys())[-1]:
                    await asyncio.sleep(random.uniform(10, 20))
                    
            except Exception as e:
                logger.error(f"Error crawling category {category}: {e}")
                results[category] = []
        
        total_articles = sum(len(articles) for articles in results.values())
        logger.info(f"Daum crawling completed. Total articles: {total_articles}")
        
        return results
    
    async def save_to_database(self, news_items: List[NewsItem]) -> Dict[str, int]:
        """ë‰´ìŠ¤ ì•„ì´í…œë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        if not news_items:
            return {"saved": 0, "duplicates": 0, "errors": 0}
        
        try:
            # NewsItemì„ DB ì €ì¥ìš© ê°ì²´ë¡œ ë³€í™˜ (DatabaseManagerê°€ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœ)
            class DBNewsItem:
                def __init__(self, title, content, source_url, source, published_at, category, thumbnail=None):
                    self.title = title
                    self.content = content
                    self.source_url = source_url
                    self.source = source
                    self.published_at = published_at
                    self.category = category
                    self.thumbnail = thumbnail
            
            db_news_items = []
            for item in news_items:
                db_news_items.append(DBNewsItem(
                    title=item.title,
                    content=item.content,
                    source_url=item.url,
                    source=item.source,
                    published_at=item.publish_date,
                    category=item.category,
                    thumbnail=item.thumbnail
                ))
            
            result = self.db_manager.save_news_batch(db_news_items)
            logger.info(f"DB ì €ì¥ ê²°ê³¼: {result}")
            return result
            
        except Exception as e:
            logger.error(f"DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"saved": 0, "duplicates": 0, "errors": len(news_items)}
    
    async def crawl_and_save_all_categories(self, count_per_category: int = 3) -> Dict[str, any]:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§í•˜ê³  DBì— ì €ì¥"""
        logger.info(f"ğŸš€ ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° DB ì €ì¥ ì‹œì‘ (ì¹´í…Œê³ ë¦¬ë‹¹ {count_per_category}ê°œ)")
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        results = await self.crawl_all_categories(count_per_category)
        
        # ëª¨ë“  ë‰´ìŠ¤ ì•„ì´í…œ ìˆ˜ì§‘
        all_news = []
        for category, news_items in results.items():
            all_news.extend(news_items)
        
        # DBì— ì €ì¥
        save_result = await self.save_to_database(all_news)
        
        logger.info(f"âœ… ë‹¤ìŒ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_news)}ê°œ ìˆ˜ì§‘, {save_result['saved']}ê°œ ì €ì¥")
        
        return {
            "crawled": len(all_news),
            "saved": save_result['saved'],
            "duplicates": save_result['duplicates'],
            "errors": save_result['errors'],
            "results": results
        }

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    crawler = DaumMobileCrawler()
    
    try:
        # DB ì €ì¥ì„ í¬í•¨í•œ ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
        result = await crawler.crawl_and_save_all_categories(3)
        
        print(f"\nğŸ¯ ë‹¤ìŒ í¬ë¡¤ë§ ë° DB ì €ì¥ ê²°ê³¼:")
        print(f"   ğŸ“Š í¬ë¡¤ë§: {result['crawled']}ê°œ")
        print(f"   ğŸ’¾ ì €ì¥: {result['saved']}ê°œ") 
        print(f"   ğŸ”„ ì¤‘ë³µ: {result['duplicates']}ê°œ")
        print(f"   âŒ ì˜¤ë¥˜: {result['errors']}ê°œ")
        
        print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸:")
        for category, articles in result['results'].items():
            print(f"   {category}: {len(articles)}ê°œ")
            for article in articles[:1]:  # ê° ì¹´í…Œê³ ë¦¬ ì²« ë²ˆì§¸ë§Œ ì¶œë ¥
                print(f"     - {article.title[:50]}...")
                
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        await crawler.close_session()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())