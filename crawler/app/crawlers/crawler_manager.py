"""
í†µí•© í¬ë¡¤ë§ ë§¤ë‹ˆì €
- ë„¤ì´ë²„/ë‹¤ìŒ ëª¨ë°”ì¼, êµ¬ê¸€ RSS í¬ë¡¤ë§ í†µí•© ê´€ë¦¬
- 2ì‹œê°„ ê°„ê²© ë¶„ì‚° ìŠ¤ì¼€ì¤„ë§ (ë„¤ì´ë²„: ì •ì‹œ, ë‹¤ìŒ: 20ë¶„, êµ¬ê¸€: 40ë¶„)
- ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ í¬ë¡¤ë§ â†’ DB ì €ì¥ (AI ë¶„ì„ì€ ê´€ë¦¬ìì—ì„œ ë³„ë„ ì‹¤í–‰)
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from app.database.db_manager import DatabaseManager
from app.localization.messages import msg

# ê° í¬ë¡¤ëŸ¬ import
from .naver_crawler import NaverMobileCrawler
from .daum_crawler import DaumMobileCrawler  

from .nass_crawler import NassApiCrawler

# ìŠ¤ì¼€ì¤„ë§
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
import pytz

logger = logging.getLogger(__name__)

class NewsItem:
    """ê³µí†µ ë‰´ìŠ¤ ì•„ì´í…œ í´ë˜ìŠ¤"""
    def __init__(self, title: str, content: str, source: str, source_url: str, 
                 published_at: datetime, category: str = None, thumbnail: str = None):
        self.title = title
        self.content = content
        self.source = source
        self.source_url = source_url
        self.published_at = published_at
        self.category = category
        self.thumbnail = thumbnail

class UnifiedCrawlerManager:
    def __init__(self, database_url: str = None):
        """í†µí•© í¬ë¡¤ë§ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        self.database_url = database_url
        # DatabaseManager ì´ˆê¸°í™” (í•­ìƒ í•„ìš”)
        self.db_manager = DatabaseManager()
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        self.naver_crawler = NaverMobileCrawler()
        self.daum_crawler = DaumMobileCrawler()

        self.nass_crawler = NassApiCrawler()
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        self.scheduler = AsyncIOScheduler(timezone=pytz.timezone('Asia/Seoul'))
        
        # í¬ë¡¤ë§ ì„¤ì • (í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ì¤‘ì‹¬)
        self.articles_per_category = 10  # í—¤ë“œë¼ì¸ ë‰´ìŠ¤ 10ê°œì”©
        self.crawl_interval_minutes = 5  # ê° ê¸°ì‚¬ ê°„ê²©
        
        # ì§€ì› ì¹´í…Œê³ ë¦¬
        self.categories = [
            "politics", "economy", "society", "technology", 
            "world", "entertainment", "sports", "environment"
        ]
    
    def convert_mobile_to_standard_newsitem(self, mobile_item, source_prefix: str) -> NewsItem:
        """ëª¨ë°”ì¼ í¬ë¡¤ëŸ¬ NewsItemì„ í‘œì¤€ NewsItemìœ¼ë¡œ ë³€í™˜"""
        # ì‹¤ì œ ì–¸ë¡ ì‚¬ëª…ì´ ì¶”ì¶œë˜ì—ˆìœ¼ë©´ ì‚¬ìš© - ê° í¬ë¡¤ëŸ¬ì—ì„œ ì´ë¯¸ ì¶”ì¶œ ì™„ë£Œë¨
        actual_source = getattr(mobile_item, 'source', None)
        if not actual_source:  # source ì†ì„±ì´ ì—†ëŠ” ê²½ìš°ë§Œ í´ë°±
            actual_source = f"{source_prefix}ë‰´ìŠ¤"  # ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±
        # "ì•Œìˆ˜ì—†ìŒ"ì€ ìœ íš¨í•œ ì¶”ì¶œ ê²°ê³¼ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            
        # ì¸ë„¤ì¼ ì •ë³´ í™•ì¸
        thumbnail = getattr(mobile_item, 'thumbnail', None)
            
        return NewsItem(
            title=mobile_item.title,
            content=mobile_item.content,
            source=actual_source,  # ì‹¤ì œ ì¶”ì¶œëœ ì–¸ë¡ ì‚¬ëª… ì‚¬ìš©
            source_url=mobile_item.url,  # DaumMobileCrawlerëŠ” url ì†ì„± ì‚¬ìš©
            published_at=mobile_item.publish_date,
            category=mobile_item.category,
            thumbnail=thumbnail  # ì¸ë„¤ì¼ ì •ë³´ ì¶”ê°€
        )
    
    async def crawl_naver_news(self) -> Dict[str, int]:
        """ë„¤ì´ë²„ ëª¨ë°”ì¼ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        start_time = datetime.now()
        logger.info(f"ğŸ”¥ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ - {start_time}")
        
        try:
            # ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
            results = await self.naver_crawler.crawl_all_categories(self.articles_per_category)
            
            total_saved = 0
            category_stats = {}
            
            for category, mobile_items in results.items():
                # ëª¨ë°”ì¼ NewsItemì„ í‘œì¤€ NewsItemìœ¼ë¡œ ë³€í™˜
                standard_items = [
                    self.convert_mobile_to_standard_newsitem(item, "ë„¤ì´ë²„")
                    for item in mobile_items
                ]
                
                if standard_items:
                    if self.db_manager:
                        result = self.db_manager.save_news_batch(standard_items)
                        saved = result.get('saved', 0)
                    else:
                        saved = await self.save_to_factlab_db(standard_items, "naver")
                    
                    total_saved += saved
                    category_stats[category] = {
                        'crawled': len(mobile_items),
                        'saved': saved
                    }
                    logger.info(f"ë„¤ì´ë²„ {category}: {saved}ê°œ ì €ì¥")
                else:
                    category_stats[category] = {'crawled': 0, 'saved': 0}
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            logger.info(f"âœ… ë„¤ì´ë²„ í¬ë¡¤ë§ ì™„ë£Œ - ì´ {total_saved}ê°œ ì €ì¥, {duration:.1f}ì´ˆ ì†Œìš”")
            
            # ë¡œê·¸ ì €ì¥
            await self.log_crawl_result("naver", total_saved, duration, "SUCCESS", category_stats)
            
            return {
                'source': 'naver',
                'total_saved': total_saved,
                'duration': duration,
                'categories': category_stats
            }
            
        except Exception as e:
            logger.error(f"âŒ ë„¤ì´ë²„ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            await self.log_crawl_result("naver", 0, 0, f"ERROR: {str(e)}")
            return {'source': 'naver', 'total_saved': 0, 'error': str(e)}
    
    async def crawl_daum_news(self) -> Dict[str, int]:
        """ë‹¤ìŒ ëª¨ë°”ì¼ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        start_time = datetime.now()
        logger.info(f"ğŸ”¥ ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ - {start_time}")
        
        try:
            # ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
            results = await self.daum_crawler.crawl_all_categories(self.articles_per_category)
            
            total_saved = 0
            category_stats = {}
            
            for category, mobile_items in results.items():
                # ëª¨ë°”ì¼ NewsItemì„ í‘œì¤€ NewsItemìœ¼ë¡œ ë³€í™˜
                standard_items = [
                    self.convert_mobile_to_standard_newsitem(item, "ë‹¤ìŒ")
                    for item in mobile_items
                ]
                
                if standard_items:
                    if self.db_manager:
                        result = self.db_manager.save_news_batch(standard_items)
                        saved = result.get('saved', 0)
                    else:
                        saved = await self.save_to_factlab_db(standard_items, "daum")
                    
                    total_saved += saved
                    category_stats[category] = {
                        'crawled': len(mobile_items),
                        'saved': saved
                    }
                    logger.info(f"ë‹¤ìŒ {category}: {saved}ê°œ ì €ì¥")
                else:
                    category_stats[category] = {'crawled': 0, 'saved': 0}
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            logger.info(f"âœ… ë‹¤ìŒ í¬ë¡¤ë§ ì™„ë£Œ - ì´ {total_saved}ê°œ ì €ì¥, {duration:.1f}ì´ˆ ì†Œìš”")
            
            # ë¡œê·¸ ì €ì¥
            await self.log_crawl_result("daum", total_saved, duration, "SUCCESS", category_stats)
            
            return {
                'source': 'daum',
                'total_saved': total_saved,
                'duration': duration,
                'categories': category_stats
            }
            
        except Exception as e:
            logger.error(f"âŒ ë‹¤ìŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            await self.log_crawl_result("daum", 0, 0, f"ERROR: {str(e)}")
            return {'source': 'daum', 'total_saved': 0, 'error': str(e)}
    


    async def crawl_bills(self, days: int = 30) -> Dict[str, int]:
        """êµ­íšŒ ë²•ì•ˆ í¬ë¡¤ë§"""
        start_time = datetime.now()
        logger.info(f"ğŸ”¥ êµ­íšŒ ë²•ì•ˆ í¬ë¡¤ë§ ì‹œì‘ - {start_time}")
        
        try:
            async with self.nass_crawler as crawler:
                bills = await crawler.crawl_recent_bills(days=days)
                
                if self.db_manager:
                    result = self.db_manager.save_bills_batch(bills)
                    saved = result.get('saved', 0)
                else:
                    saved = 0

                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                
                logger.info(f"âœ… êµ­íšŒ ë²•ì•ˆ í¬ë¡¤ë§ ì™„ë£Œ - ì´ {saved}ê°œ ì €ì¥, {duration:.1f}ì´ˆ ì†Œìš”")
                
                await self.log_crawl_result("nass", saved, duration, "SUCCESS")
                
                return {
                    'source': 'nass',
                    'total_saved': saved,
                    'duration': duration
                }

        except Exception as e:
            logger.error(f"âŒ êµ­íšŒ ë²•ì•ˆ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            await self.log_crawl_result("nass", 0, 0, f"ERROR: {str(e)}")
            return {'source': 'nass', 'total_saved': 0, 'error': str(e)}
    
    async def save_to_factlab_db(self, news_items: List[NewsItem], source: str) -> int:
        """FactLab ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (PostgreSQL ì§ì ‘ ì—°ê²°)"""
        if not self.database_url:
            logger.warning("Database URL not provided, skipping save")
            return 0
        
        import psycopg2
        from psycopg2.extras import RealDictCursor
        
        conn = psycopg2.connect(self.database_url, cursor_factory=RealDictCursor)
        saved_count = 0
        
        try:
            cursor = conn.cursor()
            
            for news_item in news_items:
                try:
                    # ì¤‘ë³µ ì²´í¬ (URL ê¸°ì¤€)
                    cursor.execute("SELECT id FROM news WHERE url = %s", (news_item.source_url,))
                    existing = cursor.fetchone()
                    
                    if existing:
                        logger.debug(f"ì¤‘ë³µ ë‰´ìŠ¤ ê±´ë„ˆë›°ê¸°: {news_item.source_url}")
                        continue
                    
                    # ë‰´ìŠ¤ ì‚½ì…
                    cursor.execute("""
                        INSERT INTO news (title, content, url, source, publish_date, category, status, created_at, updated_at, thumbnail)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        RETURNING id
                    """, (
                        news_item.title,
                        news_item.content,
                        news_item.source_url,
                        news_item.source,
                        news_item.published_at,
                        news_item.category,
                        'PENDING',  # í¬ë¡¤ë§ í›„ ê´€ë¦¬ì ìŠ¹ì¸ ëŒ€ê¸°
                        datetime.now(),
                        datetime.now(),
                        news_item.thumbnail  # ì¸ë„¤ì¼ ì •ë³´ ì¶”ê°€
                    ))
                    
                    result = cursor.fetchone()
                    logger.debug(f"INSERT ì¿¼ë¦¬ ê²°ê³¼: {result}")
                    if result:
                        news_id = result['id']  # RealDictCursorëŠ” ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì ‘ê·¼
                    else:
                        logger.error("INSERT ì¿¼ë¦¬ê°€ IDë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                        continue
                    saved_count += 1
                    logger.debug(f"ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ ID {news_id}: {news_item.title[:50]}...")
                    
                except Exception as e:
                    logger.error(f"ë‰´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
                    logger.error(f"ë‰´ìŠ¤ ì •ë³´: title={news_item.title[:50]}, url={news_item.source_url}")
                    import traceback
                    logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
                    conn.rollback()
                    continue
            
            conn.commit()
            logger.info(f"{source}ì—ì„œ {saved_count}ê°œ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ")
            return saved_count
            
        except Exception as e:
            conn.rollback()
            logger.error(f"{source} ë‰´ìŠ¤ ì €ì¥ ì¤‘ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}")
            return 0
        finally:
            cursor.close()
            conn.close()
    
    async def log_crawl_result(self, source: str, articles_saved: int, duration: float, 
                              status: str, category_stats: Dict = None):
        """í¬ë¡¤ë§ ê²°ê³¼ ë¡œê·¸ ì €ì¥"""
        if not self.database_url:
            return
        
        import psycopg2
        from psycopg2.extras import RealDictCursor
        import json
        
        conn = psycopg2.connect(self.database_url, cursor_factory=RealDictCursor)
        
        try:
            cursor = conn.cursor()
            
            # í¬ë¡¤ë§ ë¡œê·¸ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS crawl_logs (
                    id SERIAL PRIMARY KEY,
                    source VARCHAR(50) NOT NULL,
                    articles_saved INTEGER DEFAULT 0,
                    duration_seconds FLOAT DEFAULT 0,
                    status VARCHAR(100) NOT NULL,
                    category_stats JSONB DEFAULT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            cursor.execute("""
                INSERT INTO crawl_logs (source, articles_saved, duration_seconds, status, category_stats, created_at)
                VALUES (%s, %s, %s, %s, %s, %s)
            """, (
                source, 
                articles_saved, 
                duration, 
                status, 
                json.dumps(category_stats) if category_stats else None,
                datetime.now()
            ))
            
            conn.commit()
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥ ì˜¤ë¥˜: {e}")
            conn.rollback()
        finally:
            cursor.close()
            conn.close()
    
    def setup_schedule(self):
        """í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ì„¤ì • - ë§¤ì¼ ì•„ì¹¨ 7ì‹œ ì‹¤í–‰"""
        logger.info("ğŸ• í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ì„¤ì • ì¤‘...")
        
        # ë„¤ì´ë²„: ë§¤ì¼ ì•„ì¹¨ 7ì‹œ 00ë¶„
        self.scheduler.add_job(
            self.crawl_naver_news,
            CronTrigger(hour=7, minute=0, second=0),
            id='naver_crawl',
            max_instances=1,
            coalesce=True
        )
        
        # ë‹¤ìŒ: ë§¤ì¼ ì•„ì¹¨ 7ì‹œ 10ë¶„
        self.scheduler.add_job(
            self.crawl_daum_news,
            CronTrigger(hour=7, minute=10, second=0),
            id='daum_crawl',
            max_instances=1,
            coalesce=True
        )
        
        logger.info("âœ… í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ì„¤ì • ì™„ë£Œ:")
        logger.info("  - ë„¤ì´ë²„: ë§¤ì¼ 07:00")
        logger.info("  - ë‹¤ìŒ: ë§¤ì¼ 07:10")
    
    async def start_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        logger.info("ğŸš€ í†µí•© í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘...")
        
        # ìŠ¤ì¼€ì¤„ ì„¤ì •
        self.setup_schedule()
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
        self.scheduler.start()
        logger.info("âœ… í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì™„ë£Œ")
        
        # ì¦‰ì‹œ ì‹¤í–‰ ì˜µì…˜ (í…ŒìŠ¤íŠ¸ìš©)
        if os.getenv("IMMEDIATE_CRAWL", "false").lower() == "true":
            logger.info("ğŸ”¥ ì¦‰ì‹œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
            await asyncio.sleep(2)
            await self.crawl_naver_news()
            await asyncio.sleep(5)
            await self.crawl_daum_news()

    
    async def stop_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
        logger.info("â¹ï¸ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€...")
        self.scheduler.shutdown()
        
        # í¬ë¡¤ëŸ¬ ì„¸ì…˜ ì •ë¦¬
        await self.naver_crawler.close_session()
        await self.daum_crawler.close_session()
        
        logger.info("âœ… í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ ì™„ë£Œ")
    
    def get_scheduler_status(self) -> Dict:
        """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì¡°íšŒ"""
        jobs = []
        for job in self.scheduler.get_jobs():
            jobs.append({
                "id": job.id,
                "name": job.name or job.id,
                "next_run": job.next_run_time.isoformat() if job.next_run_time else None,
                "trigger": str(job.trigger)
            })
        
        return {
            "running": self.scheduler.running,
            "jobs": jobs,
            "timezone": str(self.scheduler.timezone)
        }
    
    async def manual_crawl_all(self) -> Dict:
        """ìˆ˜ë™ìœ¼ë¡œ ëª¨ë“  ì†ŒìŠ¤ í¬ë¡¤ë§"""
        logger.info("ğŸ”¥ ìˆ˜ë™ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘...")
        start_time = datetime.now()
        
        results = {}
        
        # ìˆœì°¨ ì‹¤í–‰ (ë‹¤ìŒ â†’ ë„¤ì´ë²„ â†’ êµ¬ê¸€)
        results['daum'] = await self.crawl_daum_news()
        await asyncio.sleep(10)  # ê°„ê²©

        results['naver'] = await self.crawl_naver_news()

        
        end_time = datetime.now()
        total_duration = (end_time - start_time).total_seconds()
        total_saved = sum(r.get('total_saved', 0) for r in results.values())
        
        logger.info(f"âœ… ìˆ˜ë™ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ - ì´ {total_saved}ê°œ ì €ì¥, {total_duration:.1f}ì´ˆ ì†Œìš”")
        
        return {
            'total_saved': total_saved,
            'total_duration': total_duration,
            'results': results,
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat()
        }

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://factlab_user:password@localhost:5433/factlab")
    
    manager = UnifiedCrawlerManager(DATABASE_URL)
    
    try:
        # ì¦‰ì‹œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
        if len(sys.argv) > 1 and sys.argv[1] == "test":
            result = await manager.manual_crawl_all()
            print(f"í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {result}")
        else:
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰
            await manager.start_scheduler()
            
            # ë¬´í•œ ëŒ€ê¸°
            while True:
                await asyncio.sleep(60)
                status = manager.get_scheduler_status()
                logger.info(f"ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ: {status['running']}, ì‘ì—… ìˆ˜: {len(status['jobs'])}")
                
    except KeyboardInterrupt:
        logger.info("ì¤‘ë‹¨ ì‹ í˜¸ ìˆ˜ì‹ ")
    finally:
        await manager.stop_scheduler()

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    asyncio.run(main())