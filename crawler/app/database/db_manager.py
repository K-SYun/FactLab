import psycopg2
from psycopg2.extras import RealDictCursor
from typing import List, Dict, Optional
import logging
import os
from datetime import datetime
import re
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

logger = logging.getLogger(__name__)

class DatabaseManager:
    def __init__(self):
        self.connection_params = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': os.getenv('DB_PORT', '5433'),  # FactLab í¬íŠ¸
            'database': os.getenv('DB_NAME', 'factlab'),
            'user': os.getenv('DB_USER', 'factlab_user'),
            'password': os.getenv('DB_PASSWORD', 'password')  # FactLab ë¹„ë°€ë²ˆí˜¸
        }
    
    def normalize_url(self, url: str) -> str:
        """URL ì •ê·œí™” - ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´ URLì„ í‘œì¤€í™”"""
        try:
            # ë„¤ì´ë²„ ë‰´ìŠ¤ URLì—ì„œ ëŒ“ê¸€ í˜ì´ì§€ íŒ¨í„´ ì œê±°
            # ì˜ˆ: /mnews/article/comment/xxx/yyy -> /mnews/article/xxx/yyy
            url = re.sub(r'/mnews/article/comment/', '/mnews/article/', url)
            
            # URL íŒŒì‹±
            parsed = urlparse(url)
            
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±° (ì¶”ì  íŒŒë¼ë¯¸í„° ë“±)
            # ë³´ì¡´í•  íŒŒë¼ë¯¸í„°ë§Œ ì„ ë³„ì ìœ¼ë¡œ ìœ ì§€
            query_params = parse_qs(parsed.query)
            essential_params = {}
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ í•„ìš”í•œ íŒŒë¼ë¯¸í„°ë§Œ ìœ ì§€ (oid, aid)
            if 'oid' in query_params:
                essential_params['oid'] = query_params['oid']
            if 'aid' in query_params:
                essential_params['aid'] = query_params['aid']
            
            # ì •ê·œí™”ëœ ì¿¼ë¦¬ ë¬¸ìì—´ ìƒì„±
            normalized_query = urlencode(essential_params, doseq=True) if essential_params else ''
            
            # ì •ê·œí™”ëœ URL ì¬êµ¬ì„±
            normalized_url = urlunparse((
                parsed.scheme,
                parsed.netloc,
                parsed.path,
                parsed.params,
                normalized_query,
                ''  # fragment ì œê±°
            ))
            
            return normalized_url
            
        except Exception as e:
            logger.warning(f"URL normalization failed for {url}: {e}")
            return url
    
    def normalize_title(self, title: str) -> str:
        """ì œëª© ì •ê·œí™” - ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´ ì œëª©ì„ í‘œì¤€í™”"""
        if not title:
            return ""
        
        # ì œëª©ì—ì„œ íŠ¹ìˆ˜ë¬¸ìì™€ ê³µë°± ì •ë¦¬
        normalized = re.sub(r'\s+', ' ', title.strip())  # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        normalized = re.sub(r'[^\w\sê°€-í£]', '', normalized)  # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        
        return normalized.lower()
        
    def get_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë°˜í™˜"""
        try:
            conn = psycopg2.connect(**self.connection_params)
            return conn
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            raise
    
    def save_news_item(self, news_item) -> Optional[int]:
        """ë‰´ìŠ¤ ì•„ì´í…œ ì €ì¥ - ê°•í™”ëœ ì¤‘ë³µ ì²´í¬"""
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    # URL ì •ê·œí™”
                    normalized_url = self.normalize_url(news_item.source_url)
                    normalized_title = self.normalize_title(news_item.title)
                    
                    # 1ë‹¨ê³„: ì •ê·œí™”ëœ URLë¡œ ì¤‘ë³µ ì²´í¬
                    # DBì˜ ëª¨ë“  URLì„ ì •ê·œí™”í•´ì„œ ë¹„êµ
                    cursor.execute("""
                        SELECT id, title, url FROM news 
                        WHERE REGEXP_REPLACE(url, '/mnews/article/comment/', '/mnews/article/', 'g') = %s
                    """, (normalized_url,))
                    
                    existing = cursor.fetchone()
                    if existing:
                        logger.info(f"ğŸ”„ ì¤‘ë³µ ë‰´ìŠ¤ ìŠ¤í‚µ (URL): {news_item.title[:50]}... (ê¸°ì¡´ ID: {existing[0]})")
                        return None
                    
                    # 2ë‹¨ê³„: ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ (ê°™ì€ ë‚ ì§œ, ê°™ì€ ì†ŒìŠ¤)
                    if normalized_title:
                        cursor.execute("""
                            SELECT id, title, url FROM news 
                            WHERE source = %s 
                            AND DATE(publish_date) = DATE(%s)
                            AND LOWER(REGEXP_REPLACE(REGEXP_REPLACE(title, '[^\\w\\sê°€-í£]', '', 'g'), '\\s+', ' ', 'g')) = %s
                        """, (news_item.source, news_item.published_at, normalized_title))
                        
                        title_duplicate = cursor.fetchone()
                        if title_duplicate:
                            logger.info(f"ğŸ”„ ì¤‘ë³µ ë‰´ìŠ¤ ìŠ¤í‚µ (ì œëª©): {news_item.title[:50]}... (ê¸°ì¡´ ID: {title_duplicate[0]}, ê¸°ì¡´ URL: {title_duplicate[2]})")
                            return None
                    
                    # ìƒˆ ë‰´ìŠ¤ ì €ì¥ (ì •ê·œí™”ëœ URLë¡œ ì €ì¥)
                    insert_query = """
                        INSERT INTO news (title, content, url, source, publish_date, original_publish_date, category, status, created_at, thumbnail)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        RETURNING id
                    """
                    
                    # ì‹¤ì œ ë°œí–‰ ì‹œê°„ ê°’ í™•ì¸ ë° ë¡œê¹…
                    crawling_time = getattr(news_item, 'publish_date', datetime.now())
                    original_pub_date = getattr(news_item, 'original_publish_date', crawling_time)
                    
                    logger.info(f"ğŸ’¾ DB ì €ì¥ ì‹œê°„ ì •ë³´:")
                    logger.info(f"   - í¬ë¡¤ë§ ì‹œê°„: {crawling_time}")
                    logger.info(f"   - ì‹¤ì œ ë°œí–‰ ì‹œê°„: {original_pub_date}")
                    logger.info(f"   - ê°™ì€ ì‹œê°„ì¸ê°€? {crawling_time == original_pub_date}")
                    
                    cursor.execute(insert_query, (
                        news_item.title,
                        news_item.content,
                        normalized_url,  # ì •ê·œí™”ëœ URL ì €ì¥
                        news_item.source,
                        crawling_time,  # í¬ë¡¤ë§ ì‹œê°„
                        original_pub_date,  # ì‹¤ì œ ë°œí–‰ ì‹œê°„
                        news_item.category,
                        'PENDING',  # ê¸°ë³¸ ìƒíƒœ
                        datetime.now(),
                        getattr(news_item, 'thumbnail', None)  # ì¸ë„¤ì¼ì´ ì—†ëŠ” ê²½ìš° None
                    ))
                    
                    news_id = cursor.fetchone()[0]
                    conn.commit()
                    
                    logger.info(f"âœ… DB ì €ì¥ ì„±ê³µ (ID: {news_id}): {news_item.title[:50]}...")
                    return news_id
                    
        except Exception as e:
            logger.error(f"Error saving news item: {e}")
            return None
    
    def save_news_batch(self, news_items: List) -> Dict[str, int]:
        """ë‰´ìŠ¤ ë°°ì¹˜ ì €ì¥"""
        result = {
            'saved': 0,
            'duplicates': 0,
            'errors': 0
        }
        
        for news_item in news_items:
            try:
                news_id = self.save_news_item(news_item)
                if news_id:
                    result['saved'] += 1
                else:
                    result['duplicates'] += 1
            except Exception as e:
                logger.error(f"Error saving news item: {e}")
                result['errors'] += 1
        
        logger.info(f"Batch save result: {result}")
        return result
    
    def get_pending_news(self, limit: int = 10) -> List[Dict]:
        """ìŠ¹ì¸ ëŒ€ê¸° ì¤‘ì¸ ë‰´ìŠ¤ ì¡°íšŒ"""
        try:
            with self.get_connection() as conn:
                with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                    cursor.execute("""
                        SELECT id, title, content, url, source, publish_date, category, created_at
                        FROM news 
                        WHERE status = 'PENDING'
                        ORDER BY created_at DESC
                        LIMIT %s
                    """, (limit,))
                    
                    return [dict(row) for row in cursor.fetchall()]
                    
        except Exception as e:
            logger.error(f"Error getting pending news: {e}")
            return []
    
    def update_news_status(self, news_id: int, status: str, admin_id: Optional[int] = None) -> bool:
        """ë‰´ìŠ¤ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        UPDATE news 
                        SET status = %s, updated_at = %s
                        WHERE id = %s
                    """, (status, datetime.now(), news_id))
                    
                    conn.commit()
                    logger.info(f"Updated news {news_id} status to {status}")
                    return True
                    
        except Exception as e:
            logger.error(f"Error updating news status: {e}")
            return False
    
    def get_news_stats(self) -> Dict[str, int]:
        """ë‰´ìŠ¤ í†µê³„ ì¡°íšŒ"""
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT 
                            status,
                            COUNT(*) as count
                        FROM news
                        GROUP BY status
                    """)
                    
                    stats = {}
                    for row in cursor.fetchall():
                        stats[row[0]] = row[1]
                    
                    return stats
                    
        except Exception as e:
            logger.error(f"Error getting news stats: {e}")
            return {}
    
    def get_news_by_id(self, news_id: int) -> Optional[Dict]:
        """ë‰´ìŠ¤ IDë¡œ ë‰´ìŠ¤ ì¡°íšŒ"""
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT id, title, content, url, source, publish_date, 
                               category, status, created_at, updated_at
                        FROM news 
                        WHERE id = %s
                    """, (news_id,))
                    
                    result = cursor.fetchone()
                    if result:
                        columns = ['id', 'title', 'content', 'url', 'source', 'publish_date', 
                                 'category', 'status', 'created_at', 'updated_at']
                        return dict(zip(columns, result))
                    return None
                    
        except Exception as e:
            logger.error(f"Error getting news by id {news_id}: {e}")
            return None
    
    def test_connection(self) -> bool:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("SELECT 1")
                    result = cursor.fetchone()
                    logger.info("Database connection test successful")
                    return result[0] == 1
        except Exception as e:
            logger.error(f"Database connection test failed: {e}")
            return False