import psycopg2
from psycopg2.extras import RealDictCursor
from typing import List, Dict, Optional
import logging
import os
from datetime import datetime
import re
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

logger = logging.getLogger(__name__)

class DatabaseManager:
    def __init__(self):
        self.connection_params = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': os.getenv('DB_PORT', '5433'),  # FactLab í¬íŠ¸
            'database': os.getenv('DB_NAME', 'factlab'),
            'user': os.getenv('DB_USER', 'factlab_user'),
            'password': os.getenv('DB_PASSWORD', 'password')  # FactLab ë¹„ë°€ë²ˆí˜¸
        }
    
    def normalize_url(self, url: str) -> str:
        """URL ì •ê·œí™” - ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´ URLì„ í‘œì¤€í™”"""
        try:
            # ë„¤ì´ë²„ ë‰´ìŠ¤ URLì—ì„œ ëŒ“ê¸€ í˜ì´ì§€ íŒ¨í„´ ì œê±°
            # ì˜ˆ: /mnews/article/comment/xxx/yyy -> /mnews/article/xxx/yyy
            url = re.sub(r'/mnews/article/comment/', '/mnews/article/', url)
            
            # URL íŒŒì‹±
            parsed = urlparse(url)
            
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±° (ì¶”ì  íŒŒë¼ë¯¸í„° ë“±)
            # ë³´ì¡´í•  íŒŒë¼ë¯¸í„°ë§Œ ì„ ë³„ì ìœ¼ë¡œ ìœ ì§€
            query_params = parse_qs(parsed.query)
            essential_params = {}
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ í•„ìš”í•œ íŒŒë¼ë¯¸í„°ë§Œ ìœ ì§€ (oid, aid)
            if 'oid' in query_params:
                essential_params['oid'] = query_params['oid']
            if 'aid' in query_params:
                essential_params['aid'] = query_params['aid']
            
            # ì •ê·œí™”ëœ ì¿¼ë¦¬ ë¬¸ìì—´ ìƒì„±
            normalized_query = urlencode(essential_params, doseq=True) if essential_params else ''
            
            # ì •ê·œí™”ëœ URL ì¬êµ¬ì„±
            normalized_url = urlunparse((
                parsed.scheme,
                parsed.netloc,
                parsed.path,
                parsed.params,
                normalized_query,
                ''  # fragment ì œê±°
            ))
            
            return normalized_url
            
        except Exception as e:
            logger.warning(f"URL normalization failed for {url}: {e}")
            return url
    
    def normalize_title(self, title: str) -> str:
        """ì œëª© ì •ê·œí™” - ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´ ì œëª©ì„ í‘œì¤€í™”"""
        if not title:
            return ""
        
        # ì œëª©ì—ì„œ íŠ¹ìˆ˜ë¬¸ìì™€ ê³µë°± ì •ë¦¬
        normalized = re.sub(r'\s+', ' ', title.strip())  # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        normalized = re.sub(r'[^\w\sê°€-í£]', '', normalized)  # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        
        return normalized.lower()
        
    def get_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë°˜í™˜"""
        try:
            conn = psycopg2.connect(**self.connection_params)
            return conn
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            raise
    
    def save_news_item(self, news_item) -> Optional[int]:
        """ë‰´ìŠ¤ ì•„ì´í…œ ì €ì¥ - ê°•í™”ëœ ì¤‘ë³µ ì²´í¬"""
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    # URL ì •ê·œí™”
                    normalized_url = self.normalize_url(news_item.source_url)
                    normalized_title = self.normalize_title(news_item.title)
                    
                    # 1ë‹¨ê³„: ì •ê·œí™”ëœ URLë¡œ ì¤‘ë³µ ì²´í¬
                    # DBì˜ ëª¨ë“  URLì„ ì •ê·œí™”í•´ì„œ ë¹„êµ
                    cursor.execute("""
                        SELECT id, title, url FROM news 
                        WHERE REGEXP_REPLACE(url, '/mnews/article/comment/', '/mnews/article/', 'g') = %s
                    """, (normalized_url,))
                    
                    existing = cursor.fetchone()
                    if existing:
                        logger.info(f"ğŸ”„ ì¤‘ë³µ ë‰´ìŠ¤ ìŠ¤í‚µ (URL): {news_item.title[:50]}... (ê¸°ì¡´ ID: {existing[0]})")
                        return None
                    
                    # 2ë‹¨ê³„: ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ (ê°™ì€ ë‚ ì§œ, ê°™ì€ ì†ŒìŠ¤)
                    if normalized_title:
                        cursor.execute("""
                            SELECT id, title, url FROM news 
                            WHERE source = %s 
                            AND DATE(publish_date) = DATE(%s)
                            AND LOWER(REGEXP_REPLACE(REGEXP_REPLACE(title, '[^\\w\\sê°€-í£]', '', 'g'), '\\s+', ' ', 'g')) = %s
                        """, (news_item.source, news_item.publish_date, normalized_title))
                        
                        title_duplicate = cursor.fetchone()
                        if title_duplicate:
                            logger.info(f"ğŸ”„ ì¤‘ë³µ ë‰´ìŠ¤ ìŠ¤í‚µ (ì œëª©): {news_item.title[:50]}... (ê¸°ì¡´ ID: {title_duplicate[0]}, ê¸°ì¡´ URL: {title_duplicate[2]})")
                            return None
                    
                    # ìƒˆ ë‰´ìŠ¤ ì €ì¥ (ì •ê·œí™”ëœ URLë¡œ ì €ì¥)
                    insert_query = """
                        INSERT INTO news (title, content, url, source, publish_date, original_publish_date, category, status, created_at, updated_at, thumbnail, main_featured, view_count, visibility)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        RETURNING id
                    """
                    
                    # ì‹¤ì œ ë°œí–‰ ì‹œê°„ ê°’ í™•ì¸ ë° ë¡œê¹…
                    crawling_time = getattr(news_item, 'publish_date', datetime.now())
                    original_pub_date = getattr(news_item, 'original_publish_date', crawling_time)
                    
                    logger.info(f"ğŸ’¾ DB ì €ì¥ ì‹œê°„ ì •ë³´:")
                    logger.info(f"   - í¬ë¡¤ë§ ì‹œê°„: {crawling_time}")
                    logger.info(f"   - ì‹¤ì œ ë°œí–‰ ì‹œê°„: {original_pub_date}")
                    logger.info(f"   - ê°™ì€ ì‹œê°„ì¸ê°€? {crawling_time == original_pub_date}")
                    
                    # í•„ë“œ ê¸¸ì´ ì²´í¬ ë° ë¡œê¹…
                    title_len = len(news_item.title) if news_item.title else 0
                    content_len = len(news_item.content) if news_item.content else 0
                    url_len = len(normalized_url) if normalized_url else 0
                    source_len = len(news_item.source) if news_item.source else 0
                    category_len = len(news_item.category) if news_item.category else 0
                    thumbnail_len = len(getattr(news_item, 'thumbnail', '') or '') 
                    
                    logger.info(f"ğŸ“ í•„ë“œ ê¸¸ì´ ì •ë³´:")
                    logger.info(f"   - title: {title_len}/255 ì")
                    logger.info(f"   - content: {content_len} ì (TEXT)")
                    logger.info(f"   - url: {url_len}/512 ì")
                    logger.info(f"   - source: {source_len}/100 ì")
                    logger.info(f"   - category: {category_len}/50 ì")
                    logger.info(f"   - thumbnail: {thumbnail_len}/1000 ì")
                    
                    # ê¸¸ì´ ì´ˆê³¼ í•„ë“œ ì²´í¬
                    if title_len > 255:
                        logger.error(f"âŒ title ê¸¸ì´ ì´ˆê³¼: {title_len}/255")
                    if url_len > 512:
                        logger.error(f"âŒ url ê¸¸ì´ ì´ˆê³¼: {url_len}/512")
                    if source_len > 100:
                        logger.error(f"âŒ source ê¸¸ì´ ì´ˆê³¼: {source_len}/100")
                    if category_len > 50:
                        logger.error(f"âŒ category ê¸¸ì´ ì´ˆê³¼: {category_len}/50")
                    if thumbnail_len > 1000:
                        logger.error(f"âŒ thumbnail ê¸¸ì´ ì´ˆê³¼: {thumbnail_len}/1000")
                        logger.error(f"âŒ thumbnail ë‚´ìš©: {getattr(news_item, 'thumbnail', '')[:200]}...")
                    
                    cursor.execute(insert_query, (
                        news_item.title,
                        news_item.content,
                        normalized_url,  # ì •ê·œí™”ëœ URL ì €ì¥
                        news_item.source,
                        crawling_time,  # í¬ë¡¤ë§ ì‹œê°„
                        original_pub_date,  # ì‹¤ì œ ë°œí–‰ ì‹œê°„
                        news_item.category,
                        'PENDING',  # ê¸°ë³¸ ìƒíƒœ
                        datetime.now(),
                        datetime.now(),
                        getattr(news_item, 'thumbnail', None),  # ì¸ë„¤ì¼ì´ ì—†ëŠ” ê²½ìš° None
                        getattr(news_item, 'main_featured', False),
                        0, # view_count ê¸°ë³¸ê°’
                        'PUBLIC' # visibility ê¸°ë³¸ê°’
                    ))
                    
                    news_id = cursor.fetchone()[0]
                    conn.commit()
                    
                    logger.info(f"âœ… DB ì €ì¥ ì„±ê³µ (ID: {news_id}): {news_item.title[:50]}...")
                    return news_id
                    
        except Exception as e:
            logger.error(f"Error saving news item: {e}")
            return None
    
    def save_news_batch(self, news_items: List) -> Dict[str, int]:
        """ë‰´ìŠ¤ ë°°ì¹˜ ì €ì¥"""
        result = {
            'saved': 0,
            'duplicates': 0,
            'errors': 0
        }
        
        for news_item in news_items:
            try:
                news_id = self.save_news_item(news_item)
                if news_id:
                    result['saved'] += 1
                else:
                    result['duplicates'] += 1
            except Exception as e:
                logger.error(f"Error saving news item: {e}")
                result['errors'] += 1
        
        logger.info(f"Batch save result: {result}")
        return result
    
    def get_pending_news(self, limit: int = 10) -> List[Dict]:
        """ìŠ¹ì¸ ëŒ€ê¸° ì¤‘ì¸ ë‰´ìŠ¤ ì¡°íšŒ"""
        try:
            with self.get_connection() as conn:
                with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                    cursor.execute("""
                        SELECT id, title, content, url, source, publish_date, category, created_at
                        FROM news 
                        WHERE status = 'PENDING'
                        ORDER BY created_at DESC
                        LIMIT %s
                    """, (limit,))
                    
                    return [dict(row) for row in cursor.fetchall()]
                    
        except Exception as e:
            logger.error(f"Error getting pending news: {e}")
            return []
    
    def update_news_status(self, news_id: int, status: str, admin_id: Optional[int] = None) -> bool:
        """ë‰´ìŠ¤ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        UPDATE news 
                        SET status = %s, updated_at = %s
                        WHERE id = %s
                    """, (status, datetime.now(), news_id))
                    
                    conn.commit()
                    logger.info(f"Updated news {news_id} status to {status}")
                    return True
                    
        except Exception as e:
            logger.error(f"Error updating news status: {e}")
            return False
    
    def get_news_stats(self) -> Dict[str, int]:
        """ë‰´ìŠ¤ í†µê³„ ì¡°íšŒ"""
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT 
                            status,
                            COUNT(*) as count
                        FROM news
                        GROUP BY status
                    """)
                    
                    stats = {}
                    for row in cursor.fetchall():
                        stats[row[0]] = row[1]
                    
                    return stats
                    
        except Exception as e:
            logger.error(f"Error getting news stats: {e}")
            return {}
    
    def get_news_by_id(self, news_id: int) -> Optional[Dict]:
        """ë‰´ìŠ¤ IDë¡œ ë‰´ìŠ¤ ì¡°íšŒ"""
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT id, title, content, url, source, publish_date, 
                               category, status, created_at, updated_at
                        FROM news 
                        WHERE id = %s
                    """, (news_id,))
                    
                    result = cursor.fetchone()
                    if result:
                        columns = ['id', 'title', 'content', 'url', 'source', 'publish_date', 
                                 'category', 'status', 'created_at', 'updated_at']
                        return dict(zip(columns, result))
                    return None
                    
        except Exception as e:
            logger.error(f"Error getting news by id {news_id}: {e}")
            return None
    
    def test_connection(self) -> bool:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("SELECT 1")
                    result = cursor.fetchone()
                    logger.info("Database connection test successful")
                    return result[0] == 1
        except Exception as e:
            logger.error(f"Database connection test failed: {e}")
            return False
    
    def get_daum_news_with_unknown_source(self, limit: int = 100) -> List[Dict]:
        """sourceê°€ 'ì•Œìˆ˜ì—†ìŒ'ì¸ ë‹¤ìŒ ë‰´ìŠ¤ë“¤ì„ ì¡°íšŒ"""
        try:
            with self.get_connection() as conn:
                with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                    query = """
                        SELECT id, title, url, source, category, publish_date
                        FROM news 
                        WHERE url LIKE '%v.daum.net%' 
                        AND source = 'ì•Œìˆ˜ì—†ìŒ'
                        ORDER BY id DESC
                        LIMIT %s
                    """
                    cursor.execute(query, (limit,))
                    results = cursor.fetchall()
                    logger.info(f"ë‹¤ìŒ ë‰´ìŠ¤ (ì•Œìˆ˜ì—†ìŒ) ì¡°íšŒ: {len(results)}ê°œ")
                    return [dict(row) for row in results]
        except Exception as e:
            logger.error(f"ë‹¤ìŒ ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def update_news_source(self, news_id: int, new_source: str) -> bool:
        """ë‰´ìŠ¤ì˜ source í•„ë“œë¥¼ ì—…ë°ì´íŠ¸"""
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    query = """
                        UPDATE news 
                        SET source = %s, updated_at = CURRENT_TIMESTAMP
                        WHERE id = %s
                    """
                    cursor.execute(query, (new_source, news_id))
                    conn.commit()
                    
                    if cursor.rowcount > 0:
                        logger.info(f"ë‰´ìŠ¤ source ì—…ë°ì´íŠ¸ ì„±ê³µ: ID {news_id} -> {new_source}")
                        return True
                    else:
                        logger.warning(f"ì—…ë°ì´íŠ¸í•  ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: ID {news_id}")
                        return False
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ source ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ID: {news_id}): {e}")
            return False

    def save_bills_batch(self, bills: List) -> Dict[str, int]:
        """ë²•ì•ˆ ë°°ì¹˜ ì €ì¥"""
        result = {
            'saved': 0,
            'duplicates': 0,
            'errors': 0
        }
        
        for bill in bills:
            try:
                with self.get_connection() as conn:
                    with conn.cursor() as cursor:
                        # ì¤‘ë³µ ì²´í¬ (ë²•ì•ˆ ë²ˆí˜¸ ê¸°ì¤€)
                        cursor.execute("SELECT id FROM bills WHERE bill_number = %s", (bill.bill_number,))
                        existing = cursor.fetchone()
                        
                        if existing:
                            logger.info(f"ğŸ”„ ì¤‘ë³µ ë²•ì•ˆ ìŠ¤í‚µ: {bill.title[:50]}... (ê¸°ì¡´ ID: {existing[0]})")
                            result['duplicates'] += 1
                            continue
                        
                        # ìƒˆ ë²•ì•ˆ ì €ì¥
                        insert_query = """
                            INSERT INTO bills (bill_number, title, summary, proposer_name, party_name, proposal_date, status, category, committee, stage, source_url, full_text, approval_status, created_at, updated_at)
                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                            RETURNING id
                        """
                        
                        cursor.execute(insert_query, (
                            bill.bill_number,
                            bill.title,
                            bill.summary,
                            bill.proposer_name,
                            bill.party_name,
                            bill.proposal_date,
                            bill.status,
                            bill.category,
                            bill.committee,
                            bill.stage,
                            bill.source_url,
                            bill.full_text,
                            'PENDING',  # ê¸°ë³¸ ìƒíƒœ
                            datetime.now(),
                            datetime.now()
                        ))
                        
                        bill_id = cursor.fetchone()[0]
                        conn.commit()
                        
                        logger.info(f"âœ… DB ì €ì¥ ì„±ê³µ (ID: {bill_id}): {bill.title[:50]}...")
                        result['saved'] += 1

            except Exception as e:
                logger.error(f"Error saving bill item: {e}")
                result['errors'] += 1
        
        logger.info(f"Batch save result: {result}")
        return result