"""
ë„¤ì´ë²„ ëª¨ë°”ì¼ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
- ë„¤ì´ë²„ ëª¨ë°”ì¼ ì‚¬ì´íŠ¸ì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ìˆ˜ì§‘
- 2ì‹œê°„ ê°„ê²©, ê° ì¹´í…Œê³ ë¦¬ë³„ í—¤ë“œë¼ì¸ ë‰´ìŠ¤ 10ê°œì”© ìˆ˜ì§‘
"""

import asyncio
import aiohttp
import logging
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from typing import List, Dict
import re
import hashlib
import os
from urllib.parse import urljoin, quote
import random
from dataclasses import dataclass
import sys
from pathlib import Path
from PIL import Image
import io
import base64

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from app.database.db_manager import DatabaseManager

logger = logging.getLogger(__name__)

@dataclass
class NewsItem:
    title: str
    content: str
    url: str
    source: str
    category: str
    publish_date: datetime  # í¬ë¡¤ë§ ì‹œê°„
    original_publish_date: datetime = None  # ì‹¤ì œ ë‰´ìŠ¤ ë°œí–‰ ì‹œê°„
    thumbnail: str = None
    url_hash: str = None
    
    def __post_init__(self):
        if self.url_hash is None:
            self.url_hash = hashlib.md5(self.url.encode()).hexdigest()

class NaverMobileCrawler:
    def __init__(self):
        self.base_url = "https://m.news.naver.com"
        self.session = None
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.db_manager = DatabaseManager()
        
        # ë„¤ì´ë²„ ëª¨ë°”ì¼ ì¹´í…Œê³ ë¦¬ë³„ í—¤ë“œë¼ì¸ ë‰´ìŠ¤ URL (2025ë…„ í˜„ì¬ êµ¬ì¡°)
        self.categories = {
            "politics": {"code": "100", "name": "ì •ì¹˜", "url": "https://news.naver.com/section/100"},
            "economy": {"code": "101", "name": "ê²½ì œ", "url": "https://news.naver.com/section/101"},
            "society": {"code": "102", "name": "ì‚¬íšŒ", "url": "https://news.naver.com/section/102"},
            "technology": {"code": "105", "name": "IT/ê³¼í•™", "url": "https://news.naver.com/section/105"},
            "world": {"code": "104", "name": "ì„¸ê³„", "url": "https://news.naver.com/section/104"}
        }
        
        # ëª¨ë°”ì¼ User-Agent
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
    async def init_session(self):
        """HTTP ì„¸ì…˜ ì´ˆê¸°í™”"""
        if not self.session:
            timeout = aiohttp.ClientTimeout(total=30, connect=10)
            connector = aiohttp.TCPConnector(
                limit=10, 
                limit_per_host=5,
                ssl=False  # SSL ê²€ì¦ ë¹„í™œì„±í™”
            )
            self.session = aiohttp.ClientSession(
                headers=self.headers,
                timeout=timeout,
                connector=connector
            )
            logger.info("Naver mobile crawler session initialized")
    
    async def close_session(self):
        """HTTP ì„¸ì…˜ ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
            self.session = None
            logger.info("Naver mobile crawler session closed")
    
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = re.sub(r'[\r\n\t]', ' ', text)
        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£.,!?()[\]\-]', '', text)
        
        return text.strip()
    
    def _extract_video_thumbnail(self, soup: BeautifulSoup) -> str:
        """ë™ì˜ìƒ ì¸ë„¤ì¼ ì¶”ì¶œ"""
        try:
            # ìš°ì„ ìˆœìœ„ë³„ ë™ì˜ìƒ ì¸ë„¤ì¼ ì¶”ì¶œ ë°©ë²•
            
            # 1ìˆœìœ„: video íƒœê·¸ì˜ poster ì†ì„±
            video_tags = soup.find_all('video')
            for video in video_tags:
                poster = video.get('poster')
                if poster:
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if poster.startswith('/'):
                        poster = f"https://news.naver.com{poster}"
                    elif not poster.startswith('http'):
                        poster = f"https://{poster}"
                    logger.debug(f"video poster ë°œê²¬: {poster}")
                    return poster
            
            # 2ìˆœìœ„: ë„¤ì´ë²„ ë™ì˜ìƒ íŠ¹í™” íŒ¨í„´
            naver_video_selectors = [
                # ë„¤ì´ë²„ ë™ì˜ìƒ ì¸ë„¤ì¼ í´ë˜ìŠ¤
                'img[class*="video"]',
                'img[class*="thumbnail"]',
                '.video_thumb img',
                '.video_thumbnail img',
                '.media_end_video img',
                'img[src*="video"]',
                'img[src*="thumb"]'
            ]
            
            for selector in naver_video_selectors:
                img_tags = soup.select(selector)
                for img_tag in img_tags:
                    src = img_tag.get('src') or img_tag.get('data-src')
                    if src:
                        # ë™ì˜ìƒ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                        if any(keyword in src.lower() for keyword in ['video', 'thumb', 'poster']):
                            logger.debug(f"ë™ì˜ìƒ ê´€ë ¨ ì´ë¯¸ì§€ ë°œê²¬: {src}")
                            return src
            
            # 3ìˆœìœ„: og:video:thumbnail ë©”íƒ€íƒœê·¸
            meta_video_thumbnail = soup.find('meta', property='og:video:thumbnail')
            if meta_video_thumbnail and meta_video_thumbnail.get('content'):
                logger.debug(f"og:video:thumbnail ë°œê²¬: {meta_video_thumbnail['content']}")
                return meta_video_thumbnail['content']
            
            # 4ìˆœìœ„: ìœ íŠœë¸Œ ë“± ì™¸ë¶€ ë™ì˜ìƒ iframeì—ì„œ ì¸ë„¤ì¼ ì¶”ì¶œ
            iframe_tags = soup.find_all('iframe')
            for iframe in iframe_tags:
                src = iframe.get('src', '')
                if 'youtube.com' in src or 'youtu.be' in src:
                    # ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ
                    video_id = None
                    if 'embed/' in src:
                        video_id = src.split('embed/')[-1].split('?')[0]
                    elif 'v=' in src:
                        video_id = src.split('v=')[-1].split('&')[0]
                    
                    if video_id:
                        youtube_thumbnail = f"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg"
                        logger.debug(f"ìœ íŠœë¸Œ ì¸ë„¤ì¼ ìƒì„±: {youtube_thumbnail}")
                        return youtube_thumbnail
                        
                elif 'vimeo.com' in src:
                    # Vimeo ë¹„ë””ì˜¤ ID ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
                    video_id = src.split('/')[-1].split('?')[0]
                    if video_id.isdigit():
                        vimeo_thumbnail = f"https://vumbnail.com/{video_id}.jpg"
                        logger.debug(f"Vimeo ì¸ë„¤ì¼ ìƒì„±: {vimeo_thumbnail}")
                        return vimeo_thumbnail
                        
            return None
            
        except Exception as e:
            logger.error(f"ë™ì˜ìƒ ì¸ë„¤ì¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    

    def extract_thumbnail(self, soup: BeautifulSoup) -> str:
        """ì¸ë„¤ì¼ ì´ë¯¸ì§€ URL ì¶”ì¶œ (ë³¸ë¬¸ ì´ë¯¸ì§€ ìš°ì„ , ë™ì˜ìƒ ì¸ë„¤ì¼ í¬í•¨, ì–¸ë¡ ì‚¬ ë¡œê³  ì œì™¸)"""
        try:
            # ì–¸ë¡ ì‚¬ ë¡œê³ ë‚˜ ë¶€ì ì ˆí•œ ì´ë¯¸ì§€ë¥¼ í•„í„°ë§í•  í‚¤ì›Œë“œ (ê°•í™”)
            skip_keywords_url = [
                'logo', 'icon', 'button', 'banner', 'ad', 'advertisement', 'thumb', 
                'profile', 'avatar', 'favicon', 'symbol', 'emblem', 'mark', 'ci',
                'header', 'footer', 'nav', 'menu', 'bg', 'background'
            ]
            skip_keywords_alt = [
                # í•œêµ­ ì–¸ë¡ ì‚¬
                'ë¡œê³ ', 'logo', 'ì•„ì´ì½˜', 'icon', 'ê´‘ê³ ', 'ad', 'advertisement', 
                'ë°°ë„ˆ', 'banner', 'ë¸Œëœë“œ', 'brand', 'ì–¸ë¡ ì‚¬', 'news', 'ë‰´ìŠ¤',
                'ë°©ì†¡êµ­', 'tv', 'ë¼ë””ì˜¤', 'radio', 'ê¸°ì', 'reporter', 'ì•„ë‚˜ìš´ì„œ',
                'sbs', 'kbs', 'mbc', 'jtbc', 'ytn', 'tvn', 'yna', 'ì—°í•©ë‰´ìŠ¤',
                'ì¡°ì„ ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 'ë™ì•„ì¼ë³´', 'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸', 'í•œêµ­ì¼ë³´',
                'ë§¤ì¼ê²½ì œ', 'í•œêµ­ê²½ì œ', 'ì„œìš¸ì‹ ë¬¸', 'êµ­ë¯¼ì¼ë³´', 'ë¬¸í™”ì¼ë³´', 'ì„¸ê³„ì¼ë³´',
                # ì¼ë°˜ì ì¸ UI ìš”ì†Œ
                'í”„ë¡œí•„', 'profile', 'ì¸ë„¤ì¼', 'thumbnail', 'ë¯¸ë¦¬ë³´ê¸°', 'preview',
                'íšŒì‚¬', 'company', 'ê¸°ì—…', 'corp', 'ë‹¨ì²´', 'organization'
            ]
            
            # 1ë‹¨ê³„: ë™ì˜ìƒ ì¸ë„¤ì¼ ì¶”ì¶œ ì‹œë„
            video_thumbnail = self._extract_video_thumbnail(soup)
            if video_thumbnail and self._is_valid_thumbnail(video_thumbnail, '', skip_keywords_url, skip_keywords_alt):
                logger.debug(f"ë™ì˜ìƒ ì¸ë„¤ì¼ ì¶”ì¶œ: {video_thumbnail}")
                return video_thumbnail
            
            # ìš°ì„ ìˆœìœ„ë³„ ì¸ë„¤ì¼ ì¶”ì¶œ íŒ¨í„´ (ë³¸ë¬¸ ì´ë¯¸ì§€ ìš°ì„ )
            priority_selectors = [
                # 1ìˆœìœ„: ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ (ê°€ì¥ ì¤‘ìš”)
                '.newsct_article img:not([class*="logo"]):not([class*="icon"])',
                '.end_body_wrp img:not([class*="logo"]):not([class*="icon"])',
                '#newsEndContents img:not([class*="logo"]):not([class*="icon"])',
                '._article_body_contents img:not([class*="logo"]):not([class*="icon"])',
                
                # 2ìˆœìœ„: ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸ ì´ë¯¸ì§€
                'img.end_photo_org',
                'img._LAZY_LOADING_ERROR_HIDE',
                '.media_end_head_photo img',
                '.end_photo_org',
                
                # 3ìˆœìœ„: ë©”íƒ€íƒœê·¸ ì´ë¯¸ì§€ (ìµœí›„ ìˆ˜ë‹¨)
                'meta[property="og:image"]',
                'meta[name="twitter:image"]',
                'meta[property="image"]'
            ]
            
            # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰
            for selector in priority_selectors:
                if selector.startswith('meta'):
                    meta_tag = soup.select_one(selector)
                    if meta_tag and meta_tag.get('content'):
                        img_url = meta_tag['content']
                        if self._is_valid_thumbnail(img_url, '', skip_keywords_url, skip_keywords_alt):
                            logger.debug(f"ë©”íƒ€íƒœê·¸ì—ì„œ ì¸ë„¤ì¼ ì¶”ì¶œ: {img_url}")
                            return img_url
                else:
                    img_tags = soup.select(selector)
                    for img_tag in img_tags:
                        src = img_tag.get('src') or img_tag.get('data-src')
                        alt = img_tag.get('alt', '')
                        if src and self._is_valid_thumbnail(src, alt, skip_keywords_url, skip_keywords_alt):
                            logger.debug(f"DOMì—ì„œ ì¸ë„¤ì¼ ì¶”ì¶œ: {src} (alt: {alt})")
                            return src
            
            # ëŒ€ì•ˆ: ë³¸ë¬¸ì—ì„œ í¬ê¸°ê°€ ì¶©ë¶„í•œ ì´ë¯¸ì§€ ì°¾ê¸°
            article_areas = [
                '.newsct_article', '.end_body_wrp', '#newsEndContents', '._article_body_contents'
            ]
            
            for area_selector in article_areas:
                area = soup.select_one(area_selector)
                if area:
                    images = area.find_all('img')
                    for img in images:
                        src = img.get('src') or img.get('data-src')
                        alt = img.get('alt', '')
                        width = img.get('width', '')
                        height = img.get('height', '')
                        
                        if src and self._is_valid_thumbnail(src, alt, skip_keywords_url, skip_keywords_alt):
                            # í¬ê¸° ì •ë³´ê°€ ìˆìœ¼ë©´ í™•ì¸ (ìµœì†Œ 100px)
                            if width and height:
                                try:
                                    if int(width) >= 100 and int(height) >= 100:
                                        logger.debug(f"ë³¸ë¬¸ì—ì„œ í¬ê¸° ê¸°ë°˜ ì¸ë„¤ì¼ ì¶”ì¶œ: {src}")
                                        return src
                                except:
                                    pass
                            else:
                                logger.debug(f"ë³¸ë¬¸ì—ì„œ ì¸ë„¤ì¼ ì¶”ì¶œ: {src}")
                                return src
                                
            logger.debug("ì ì ˆí•œ ì¸ë„¤ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
            
        except Exception as e:
            logger.error(f"ì¸ë„¤ì¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def extract_original_publish_date(self, soup: BeautifulSoup) -> datetime:
        """ì‹¤ì œ ë‰´ìŠ¤ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ - ê°œì„ ëœ ë²„ì „"""
        try:
            logger.debug("ğŸ•’ ì‹¤ì œ ë‰´ìŠ¤ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ ì‹œì‘")
            
            # 1ìˆœìœ„: ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© ì„ íƒìë“¤
            naver_selectors = [
                '.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME',  # ë„¤ì´ë²„ ë‰´ìŠ¤ ì‹œê°„
                '.media_end_head_info_datestamp ._ARTICLE_DATE_TIME',      # ë„¤ì´ë²„ ë‰´ìŠ¤ ì‹œê°„ (ë³€í˜•)
                '.guide_categorization_item ._ARTICLE_DATE_TIME',          # ë„¤ì´ë²„ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ì˜ì—­
                'span[data-date-time]',                                    # data-date-time ì†ì„±
                '.t11[data-date-time]',                                    # ë„¤ì´ë²„ íŠ¹ì • í´ë˜ìŠ¤
                '.date',                                                   # ì¼ë°˜ì ì¸ ë‚ ì§œ í´ë˜ìŠ¤
                '.article_info .date',                                     # ê¸°ì‚¬ ì •ë³´ ì˜ì—­ì˜ ë‚ ì§œ
                '.byline .date'                                            # ë°”ì´ë¼ì¸ì˜ ë‚ ì§œ
            ]
            
            for selector in naver_selectors:
                elements = soup.select(selector)
                for element in elements:
                    # data-date-time ì†ì„± í™•ì¸
                    date_time_attr = element.get('data-date-time')
                    if date_time_attr:
                        try:
                            # ë„¤ì´ë²„ í˜•ì‹: YYYYMMDDHHMMSS
                            if len(date_time_attr) == 14 and date_time_attr.isdigit():
                                year = int(date_time_attr[:4])
                                month = int(date_time_attr[4:6])
                                day = int(date_time_attr[6:8])
                                hour = int(date_time_attr[8:10])
                                minute = int(date_time_attr[10:12])
                                second = int(date_time_attr[12:14])
                                extracted_time = datetime(year, month, day, hour, minute, second)
                                logger.info(f"âœ… ë„¤ì´ë²„ data-date-timeì—ì„œ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ: {extracted_time}")
                                return extracted_time
                        except Exception as e:
                            logger.debug(f"data-date-time íŒŒì‹± ì‹¤íŒ¨: {e}")
                    
                    # í…ìŠ¤íŠ¸ì—ì„œ ì‹œê°„ ì¶”ì¶œ
                    text = element.get_text().strip()
                    if text:
                        extracted_time = self._parse_korean_datetime(text)
                        if extracted_time:
                            logger.info(f"âœ… ë„¤ì´ë²„ ì„ íƒìì—ì„œ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ: {extracted_time} (ì›ë³¸: {text})")
                            return extracted_time
            
            # 2ìˆœìœ„: êµ¬ì¡°í™”ëœ ë°ì´í„°ì˜ datePublished
            script_tags = soup.find_all('script', type='application/ld+json')
            for script in script_tags:
                try:
                    import json
                    data = json.loads(script.get_text())
                    if isinstance(data, dict) and 'datePublished' in data:
                        date_str = data['datePublished']
                        extracted_time = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                        logger.info(f"âœ… JSON-LDì—ì„œ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ: {extracted_time}")
                        return extracted_time
                    elif isinstance(data, list):
                        for item in data:
                            if isinstance(item, dict) and 'datePublished' in item:
                                date_str = item['datePublished']
                                extracted_time = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                                logger.info(f"âœ… JSON-LD ë°°ì—´ì—ì„œ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ: {extracted_time}")
                                return extracted_time
                except Exception as e:
                    logger.debug(f"JSON-LD íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            # 3ìˆœìœ„: ë©”íƒ€íƒœê·¸ì˜ article:published_time
            meta_published = soup.find('meta', property='article:published_time')
            if meta_published and meta_published.get('content'):
                try:
                    date_str = meta_published['content']
                    extracted_time = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                    logger.info(f"âœ… ë©”íƒ€íƒœê·¸ì—ì„œ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ: {extracted_time}")
                    return extracted_time
                except Exception as e:
                    logger.debug(f"ë©”íƒ€íƒœê·¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            # 4ìˆœìœ„: time íƒœê·¸ì˜ datetime ì†ì„±
            time_tags = soup.find_all('time')
            for time_tag in time_tags:
                datetime_attr = time_tag.get('datetime')
                if datetime_attr:
                    try:
                        extracted_time = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00'))
                        logger.info(f"âœ… time íƒœê·¸ì—ì„œ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ: {extracted_time}")
                        return extracted_time
                    except Exception as e:
                        logger.debug(f"time íƒœê·¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
            
            # 5ìˆœìœ„: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ ê²€ìƒ‰
            text = soup.get_text()
            extracted_time = self._parse_korean_datetime(text)
            if extracted_time:
                logger.info(f"âœ… ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ: {extracted_time}")
                return extracted_time
            
            # ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” ê²½ìš° í˜„ì¬ ì‹œê°„ì—ì„œ ëœë¤í•˜ê²Œ 1-12ì‹œê°„ ì „ìœ¼ë¡œ ì„¤ì •
            import random
            hours_ago = random.randint(1, 12)
            minutes_ago = random.randint(0, 59)
            fake_publish_time = datetime.now() - timedelta(hours=hours_ago, minutes=minutes_ago)
            logger.warning(f"âš ï¸ ì‹¤ì œ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ ì‹¤íŒ¨, ëœë¤ ì‹œê°„ ìƒì„±: {fake_publish_time}")
            return fake_publish_time
            
        except Exception as e:
            logger.error(f"âŒ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ì—ì„œ 2-6ì‹œê°„ ì „
            import random
            hours_ago = random.randint(2, 6)
            return datetime.now() - timedelta(hours=hours_ago)
    
    def _parse_korean_datetime(self, text: str) -> datetime:
        """í•œêµ­ì–´ ë‚ ì§œ/ì‹œê°„ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±"""
        import re
        
        # ë‹¤ì–‘í•œ í•œêµ­ì–´ ë‚ ì§œ íŒ¨í„´ë“¤
        patterns = [
            # YYYY-MM-DD HH:MM í˜•ì‹
            r'(\d{4})-(\d{1,2})-(\d{1,2})\s+(\d{1,2}):(\d{1,2})',
            # YYYY.MM.DD HH:MM í˜•ì‹  
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})\s+(\d{1,2}):(\d{1,2})',
            # YYYYë…„ MMì›” DDì¼ HH:MM í˜•ì‹
            r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼\s+(\d{1,2}):(\d{1,2})',
            # MMì›” DDì¼ HH:MM í˜•ì‹ (ì˜¬í•´)
            r'(\d{1,2})ì›”\s*(\d{1,2})ì¼\s+(\d{1,2}):(\d{1,2})',
            # ì˜¤ì „/ì˜¤í›„ ì‹œê°„ í˜•ì‹
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})\.\s*(ì˜¤ì „|ì˜¤í›„)\s*(\d{1,2}):(\d{1,2})',
            r'(\d{4})-(\d{1,2})-(\d{1,2})\s*(ì˜¤ì „|ì˜¤í›„)\s*(\d{1,2}):(\d{1,2})',
            # ë„¤ì´ë²„ íŠ¹ìˆ˜ í˜•ì‹
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})\s*(\d{1,2}):(\d{1,2})',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                try:
                    if len(match) == 5:  # YYYY-MM-DD HH:MM ë˜ëŠ” YYYY.MM.DD HH:MM
                        year, month, day, hour, minute = map(int, match)
                        return datetime(year, month, day, hour, minute)
                    elif len(match) == 4:  # MMì›” DDì¼ HH:MM (ì˜¬í•´)
                        month, day, hour, minute = map(int, match)
                        return datetime(datetime.now().year, month, day, hour, minute)
                    elif len(match) == 6:  # ì˜¤ì „/ì˜¤í›„ í˜•ì‹
                        if match[3] in ['ì˜¤ì „', 'ì˜¤í›„']:  # ì˜¤ì „/ì˜¤í›„ ìˆëŠ” ê²½ìš°
                            year, month, day, ampm, hour, minute = match
                            year, month, day, hour, minute = int(year), int(month), int(day), int(hour), int(minute)
                            if ampm == 'ì˜¤í›„' and hour != 12:
                                hour += 12
                            elif ampm == 'ì˜¤ì „' and hour == 12:
                                hour = 0
                            return datetime(year, month, day, hour, minute)
                        else:  # ì¼ë°˜ ì‹œê°„
                            year, month, day, hour, minute = map(int, match[:5])
                            return datetime(year, month, day, hour, minute)
                except (ValueError, IndexError) as e:
                    logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {match}, ì—ëŸ¬: {e}")
                    continue
        
        # ìƒëŒ€ ì‹œê°„ (ëª‡ ì‹œê°„ ì „, ëª‡ ë¶„ ì „)
        relative_patterns = [
            r'(\d+)ì‹œê°„\s*ì „',
            r'(\d+)ë¶„\s*ì „',
            r'(\d+)ì¼\s*ì „'
        ]
        
        for pattern in relative_patterns:
            match = re.search(pattern, text)
            if match:
                amount = int(match.group(1))
                now = datetime.now()
                if 'ì‹œê°„' in pattern:
                    return now - timedelta(hours=amount)
                elif 'ë¶„' in pattern:
                    return now - timedelta(minutes=amount)
                elif 'ì¼' in pattern:
                    return now - timedelta(days=amount)
        
        return None
    
    def _is_valid_thumbnail(self, src: str, alt: str, skip_keywords_url: list, skip_keywords_alt: list) -> bool:
        """ì¸ë„¤ì¼ì´ ì ì ˆí•œì§€ ê²€ì‚¬ (URLê³¼ alt ê°’ ê¸°ì¤€) - ê°•í™”ëœ í•„í„°ë§"""
        try:
            # URL ìœ íš¨ì„± ê²€ì‚¬
            if not src or not src.startswith(('http://', 'https://', 'data:')):
                return False
            
            src_lower = src.lower()
            alt_lower = alt.lower()
            
            # URLì— ë¶€ì ì ˆí•œ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê²€ì‚¬ (ê°•í™”)
            for keyword in skip_keywords_url:
                if keyword in src_lower:
                    logger.debug(f"URL í‚¤ì›Œë“œ í•„í„°ë§: {keyword} in {src}")
                    return False
            
            # alt ê°’ì— ë¶€ì ì ˆí•œ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê²€ì‚¬ (ê°•í™”)  
            for keyword in skip_keywords_alt:
                if keyword in alt_lower:
                    logger.debug(f"ALT í‚¤ì›Œë“œ í•„í„°ë§: {keyword} in {alt}")
                    return False
            
            # íŒŒì¼ í™•ì¥ì ê²€ì‚¬ (ì´ë¯¸ì§€ íŒŒì¼ë§Œ)
            if not any(ext in src_lower for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                # data: URLì˜ ê²½ìš°ëŠ” í—ˆìš©
                if not src.startswith('data:image/'):
                    logger.debug(f"ì´ë¯¸ì§€ íŒŒì¼ í™•ì¥ìê°€ ì•„ë‹˜: {src}")
                    return False
            
            # ì´ë¯¸ì§€ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ì€ ê²ƒ ì œì™¸ (URLì—ì„œ í¬ê¸° ì •ë³´ í™•ì¸)
            small_sizes = [
                '16x16', '24x24', '32x32', '48x48', '64x64', '80x80',
                '16_16', '24_24', '32_32', '48_48', '64_64', '80_80',
                'thumb', 'small', 'mini', 'tiny'
            ]
            if any(size in src_lower for size in small_sizes):
                logger.debug(f"ì´ë¯¸ì§€ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {src}")
                return False
            
            # ë„¤ì´ë²„ íŠ¹ì • ë¡œê³ /ì•„ì´ì½˜ íŒ¨í„´ ì œì™¸
            naver_logo_patterns = [
                'static.nimg.jp', 'imgnews.pstatic.net/image/logo',
                'ssl.pstatic.net/static', 'phinf.pstatic.net/contact',
                'dthumb-phinf.pstatic.net', 'logoimg'
            ]
            if any(pattern in src_lower for pattern in naver_logo_patterns):
                logger.debug(f"ë„¤ì´ë²„ ë¡œê³  íŒ¨í„´ í•„í„°ë§: {src}")
                return False
            
            # ìµœì†Œ í’ˆì§ˆ ê¸°ì¤€: URLì— resolutionì´ë‚˜ width ì •ë³´ê°€ ìˆëŠ” ê²½ìš° í™•ì¸
            resolution_patterns = ['w=', 'width=', 'size=', 'res=']
            for pattern in resolution_patterns:
                if pattern in src_lower:
                    try:
                        # w=100 ê°™ì€ íŒ¨í„´ì—ì„œ í¬ê¸° ì¶”ì¶œ
                        import re
                        match = re.search(f'{pattern}(\\d+)', src_lower)
                        if match:
                            size = int(match.group(1))
                            if size < 120:  # ìµœì†Œ 120px
                                logger.debug(f"URLì—ì„œ ì¶”ì¶œí•œ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {size}px")
                                return False
                    except:
                        pass
                        
            return True
            
        except Exception as e:
            logger.debug(f"ì¸ë„¤ì¼ ìœ íš¨ì„± ê²€ì‚¬ ì˜¤ë¥˜: {e}")
            return False
    
    async def resize_and_optimize_image(self, image_url: str, 
                                      mobile_size: tuple = (200, 150), 
                                      desktop_size: tuple = (400, 300), 
                                      retina_size: tuple = (800, 600)) -> str:
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ, ë¦¬ì‚¬ì´ì¦ˆ í›„ íŒŒì¼ë¡œ ì €ì¥ - íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        try:
            if not image_url:
                return None
                
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            headers = {
                'User-Agent': self.user_agent,
                'Referer': 'https://news.naver.com/'
            }
            
            async with self.session.get(image_url, headers=headers, timeout=10) as response:
                if response.status != 200:
                    logger.debug(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({image_url}): HTTP {response.status}")
                    return image_url
                    
                image_data = await response.read()
                
                # ì´ë¯¸ì§€ í¬ê¸° ì²´í¬ (10MB ì œí•œ)
                if len(image_data) > 10 * 1024 * 1024:
                    logger.debug(f"ì´ë¯¸ì§€ í¬ê¸° ì´ˆê³¼ ({image_url}): {len(image_data)} bytes")
                    return image_url
                
            # PILë¡œ ì´ë¯¸ì§€ ì²˜ë¦¬
            image = Image.open(io.BytesIO(image_data))
            
            # RGBA ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜ (JPEG ì €ì¥ì„ ìœ„í•¨)
            if image.mode in ('RGBA', 'LA', 'P'):
                # í°ìƒ‰ ë°°ê²½ìœ¼ë¡œ ë³€í™˜
                background = Image.new('RGB', image.size, (255, 255, 255))
                if image.mode == 'P':
                    image = image.convert('RGBA')
                background.paste(image, mask=image.split()[-1] if image.mode == 'RGBA' else None)
                image = background
            
            # ë°ìŠ¤í¬íƒ‘ìš© ì¸ë„¤ì¼ ìƒì„± (400x300, ê³ í•´ìƒë„ëŠ” 800x600)
            image_desktop = image.copy()
            image_desktop.thumbnail(desktop_size, Image.Resampling.LANCZOS)
            
            # ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„± (URL í•´ì‹œ + íƒ€ì„ìŠ¤íƒ¬í”„)
            import hashlib
            url_hash = hashlib.md5(image_url.encode()).hexdigest()[:8]
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"thumb_{url_hash}_{timestamp}.jpg"
            
            # ì¸ë„¤ì¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            thumb_dir = "/app/thumbnails"
            os.makedirs(thumb_dir, exist_ok=True)
            
            # ë°ìŠ¤í¬íƒ‘ìš© ì¸ë„¤ì¼ ì €ì¥
            desktop_path = os.path.join(thumb_dir, filename)
            image_desktop.save(desktop_path, format='JPEG', quality=85, optimize=True)
            
            # ì›¹ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ ê²½ë¡œ ë°˜í™˜ (nginxë¥¼ í†µí•´ ì„œë¹™)
            web_path = f"/thumbnails/{filename}"
            logger.debug(f"ì¸ë„¤ì¼ ì €ì¥ ì™„ë£Œ: {web_path} (í¬ê¸°: {image_desktop.size})")
            
            return web_path
            
        except Exception as e:
            logger.debug(f"ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨ ({image_url}): {e}")
            return image_url  # ì›ë³¸ URL ë°˜í™˜
    
    async def get_category_news_list(self, category: str, count: int = 10) -> List[str]:
        """ì¹´í…Œê³ ë¦¬ë³„ í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ëª©ë¡ URL ìˆ˜ì§‘ (10ê°œ í•œì •)"""
        if category not in self.categories:
            logger.error(f"Invalid category: {category}")
            return []
        
        category_info = self.categories[category]
        section_url = category_info['url']
        
        try:
            await asyncio.sleep(random.uniform(1, 3))  # ëœë¤ ì§€ì—°
            
            async with self.session.get(section_url) as response:
                if response.status != 200:
                    logger.error(f"Failed to fetch {section_url}: {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ì˜ì—­ ìš°ì„  ê²€ìƒ‰
                news_links = []
                
                # ë„¤ì´ë²„ ë‰´ìŠ¤ ì„ íƒìë“¤ (ì‹¤ì œ ê¸°ì‚¬ ë§í¬ ìš°ì„ )
                headline_selectors = [
                    # 1ìˆœìœ„: ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ (mnews)
                    'a[href*="/mnews/article/"]',
                    'a[href*="n.news.naver.com"]',
                    # 2ìˆœìœ„: ì§ì ‘ ë‰´ìŠ¤ ë§í¬
                    'a[href*="/read/"]',
                    'a[href*="oid="][href*="aid="]',
                    # 3ìˆœìœ„: ì¼ë°˜ ë‰´ìŠ¤ ì˜ì—­
                    '.section_headline a',
                    '.headline_list a',
                    '.main_news a',
                    '.news_list a'
                ]
                
                # í—¤ë“œë¼ì¸ ì˜ì—­ì—ì„œ ìš°ì„  ê²€ìƒ‰
                for selector in headline_selectors:
                    elements = soup.select(selector)
                    for element in elements:
                        href = element.get('href')
                        if not href:
                            continue
                            
                        # ëŒ“ê¸€ í˜ì´ì§€ ì œì™¸
                        if '/comment/' in href or '/comment?' in href:
                            continue
                        
                        # ë„¤ì´ë²„ ë‰´ìŠ¤ URL íŒ¨í„´ í™•ì¸ (ìƒˆë¡œìš´ êµ¬ì¡° í¬í•¨)
                        if any(pattern in href for pattern in ['/mnews/article/', '/read/', 'oid=', 'aid=', 'article']):
                            if href.startswith('http'):
                                news_links.append(href)
                            elif href.startswith('/'):
                                # ìƒˆë¡œìš´ ë„¤ì´ë²„ ë‰´ìŠ¤ ë„ë©”ì¸ êµ¬ì¡° ì ìš©
                                if '/mnews/article/' in href:
                                    news_links.append(f"https://n.news.naver.com{href}")
                                else:
                                    news_links.append(f"https://news.naver.com{href}")
                    
                    # í—¤ë“œë¼ì¸ì—ì„œ ì¶©ë¶„í•œ ë§í¬ë¥¼ ì°¾ì•˜ë‹¤ë©´ ì¤‘ë‹¨
                    if len(news_links) >= count:
                        break
                
                # í—¤ë“œë¼ì¸ì—ì„œ ì¶©ë¶„íˆ ì°¾ì§€ ëª»í•œ ê²½ìš°, ì¼ë°˜ ë‰´ìŠ¤ë¡œ ë³´ì™„
                if len(news_links) < count:
                    logger.info(f"í—¤ë“œë¼ì¸ì—ì„œ {len(news_links)}ê°œë§Œ ì°¾ìŒ. ì¼ë°˜ ë‰´ìŠ¤ë¡œ ë³´ì™„ ì¤‘...")
                    links = soup.find_all('a', href=True)
                    for link in links:
                        if len(news_links) >= count:
                            break
                            
                        href = link['href']
                        title_text = link.get_text(strip=True)
                        
                        # ì´ë¯¸ ì¶”ê°€ëœ ë§í¬ëŠ” ìŠ¤í‚µ
                        if href in news_links:
                            continue
                        
                        # ëŒ“ê¸€ í˜ì´ì§€ ì œì™¸
                        if '/comment/' in href or '/comment?' in href:
                            continue
                        
                        # ì œëª©ì´ ìˆê³  ë‰´ìŠ¤ ê´€ë ¨ ë§í¬ì¸ ê²½ìš°
                        if title_text and len(title_text) > 15:
                            if any(pattern in href for pattern in ['/mnews/article/', '/read/', 'oid=', 'aid=', 'article']):
                                if href.startswith('http'):
                                    news_links.append(href)
                                elif href.startswith('/'):
                                    # ìƒˆë¡œìš´ ë„¤ì´ë²„ ë‰´ìŠ¤ ë„ë©”ì¸ êµ¬ì¡° ì ìš©
                                    if '/mnews/article/' in href:
                                        news_links.append(f"https://n.news.naver.com{href}")
                                    else:
                                        news_links.append(f"https://news.naver.com{href}")
                
                # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
                unique_links = []
                seen = set()
                for link in news_links:
                    if link not in seen:
                        unique_links.append(link)
                        seen.add(link)
                
                final_links = unique_links[:count]  # ì •í™•íˆ 10ê°œë§Œ
                
                if final_links:
                    logger.info(f"Found {len(final_links)} headline news links for category {category}")
                    return final_links
                else:
                    logger.warning(f"No headline news links found for category {category}")
                    return []
                        
        except Exception as e:
            logger.error(f"Error fetching headline news from {section_url}: {e}")
            return []
    
    async def crawl_news_article(self, url: str, category: str) -> NewsItem:
        """ê°œë³„ ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§"""
        try:
            await asyncio.sleep(5)  # 5ì´ˆ ê°„ê²©
            
            async with self.session.get(url) as response:
                if response.status != 200:
                    logger.error(f"Failed to fetch article: {response.status}")
                    return None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # ì œëª© ì¶”ì¶œ (2025ë…„ ë„¤ì´ë²„ ë‰´ìŠ¤ êµ¬ì¡°)
                title_elem = soup.select_one('h2.media_end_head_headline span') or \
                           soup.find('h2', class_='media_end_head_headline') or \
                           soup.find('h1', class_='media_end_head_headline') or \
                           soup.find('h1', class_='end_tit') or \
                           soup.select_one('#title_area span') or \
                           soup.find('h1') or \
                           soup.find('h2')
                
                title = self.clean_text(title_elem.get_text()) if title_elem else "ì œëª© ì—†ìŒ"
                
                # ë³¸ë¬¸ ì¶”ì¶œ (2025ë…„ ë„¤ì´ë²„ ë‰´ìŠ¤ êµ¬ì¡°)
                content_elem = soup.find('div', class_='newsct_article _article_body') or \
                              soup.find('div', class_='newsct_article') or \
                              soup.find('div', class_='_article_body') or \
                              soup.find('div', class_='media_end_body_cont') or \
                              soup.find('div', class_='end_body_wrp') or \
                              soup.find('div', id='newsEndContents') or \
                              soup.find('div', class_='_article_body_contents')
                
                content = ""
                if content_elem:
                    # ìŠ¤í¬ë¦½íŠ¸, ê´‘ê³  ë“± ì œê±°
                    for unwanted in content_elem.find_all(['script', 'style', 'iframe', 'ins']):
                        unwanted.decompose()
                    content = self.clean_text(content_elem.get_text())
                
                if not content:
                    content = "ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                # ê¸°ì‚¬ ê¸¸ì´ ì œí•œ
                if len(content) > 2000:
                    content = content[:2000] + "..."
                
                # ì¸ë„¤ì¼ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ë¦¬ì‚¬ì´ì¦ˆ
                thumbnail_url = self.extract_thumbnail(soup)
                if thumbnail_url:
                    thumbnail_url = await self.resize_and_optimize_image(thumbnail_url)
                
                # ì‹¤ì œ ë‰´ìŠ¤ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ
                original_publish_date = self.extract_original_publish_date(soup)
                crawling_time = datetime.now()
                
                logger.info(f"ğŸ“° ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„±:")
                logger.info(f"   - í¬ë¡¤ë§ ì‹œê°„: {crawling_time}")
                logger.info(f"   - ì¶”ì¶œëœ ë°œí–‰ ì‹œê°„: {original_publish_date}")
                logger.info(f"   - ê°™ì€ ì‹œê°„ì¸ê°€? {crawling_time.replace(microsecond=0) == original_publish_date}")
                
                # ìˆ˜ì§‘ëœ ì¹´í…Œê³ ë¦¬ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í˜ì´ì§€ì—ì„œ ìˆ˜ì§‘ë˜ì—ˆìœ¼ë¯€ë¡œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜)
                final_category = category
                
                news_item = NewsItem(
                    title=title,
                    content=content,
                    url=url,
                    source="ë„¤ì´ë²„ë‰´ìŠ¤",
                    category=final_category,
                    publish_date=crawling_time,  # í¬ë¡¤ë§ ì‹œê°„
                    original_publish_date=original_publish_date,  # ì‹¤ì œ ë°œí–‰ ì‹œê°„
                    thumbnail=thumbnail_url
                )
                
                # ìƒì„±ëœ ë‰´ìŠ¤ ì•„ì´í…œ í™•ì¸
                logger.info(f"ğŸ“ ìƒì„±ëœ NewsItem:")
                logger.info(f"   - publish_date: {news_item.publish_date}")
                logger.info(f"   - original_publish_date: {news_item.original_publish_date}")
                
                logger.info(f"Successfully crawled: {title[:50]}...")
                return news_item
                
        except Exception as e:
            logger.error(f"Error crawling article {url}: {e}")
            return None
    
    async def crawl_category(self, category: str, count: int = 10) -> List[NewsItem]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        logger.info(f"Starting crawl for category: {category} (target: {count} articles)")
        
        await self.init_session()
        
        try:
            # 1. ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘
            news_urls = await self.get_category_news_list(category, count)
            
            if not news_urls:
                logger.warning(f"No news URLs found for category: {category}")
                return []
            
            # 2. ê°œë³„ ê¸°ì‚¬ í¬ë¡¤ë§ ë° ì¦‰ì‹œ DB ì €ì¥ (5ì´ˆ ê°„ê²©)
            news_items = []
            saved_count = 0
            for i, url in enumerate(news_urls):
                logger.info(f"Crawling article {i+1}/{len(news_urls)}: {url}")
                
                news_item = await self.crawl_news_article(url, category)
                if news_item:
                    news_items.append(news_item)
                    # ì¦‰ì‹œ DBì— ì €ì¥
                    if await self.save_news_to_db(news_item):
                        saved_count += 1
                
                # 5ì´ˆ ê°„ê²©
                await asyncio.sleep(5)
                
                # ì§„í–‰ë¥  ë¡œê·¸
                if (i + 1) % 5 == 0:
                    logger.info(f"Progress: {i+1}/{len(news_urls)} articles crawled, {saved_count} saved to DB")
            
            logger.info(f"Completed crawling for {category}: {len(news_items)} articles collected, {saved_count} saved to DB")
            return news_items
            
        except Exception as e:
            logger.error(f"Error in crawl_category for {category}: {e}")
            return []
        
        finally:
            await self.close_session()
    
    async def crawl_all_categories(self, count_per_category: int = 10) -> Dict[str, List[NewsItem]]:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ í—¤ë“œë¼ì¸ ë‰´ìŠ¤ í¬ë¡¤ë§ (ê° ì¹´í…Œê³ ë¦¬ë³„ 10ê°œì”©)"""
        logger.info(f"Starting crawl for all Naver headline categories (target: {count_per_category} per category)")
        
        results = {}
        
        for category in self.categories.keys():
            logger.info(f"Starting headline crawl for category: {category}")
            try:
                news_items = await self.crawl_category(category, count_per_category)
                results[category] = news_items
                
                # ì¹´í…Œê³ ë¦¬ ê°„ ê°„ê²©
                if category != list(self.categories.keys())[-1]:
                    await asyncio.sleep(random.uniform(10, 20))
                    
            except Exception as e:
                logger.error(f"Error crawling category {category}: {e}")
                results[category] = []
        
        total_articles = sum(len(articles) for articles in results.values())
        logger.info(f"Naver crawling completed. Total articles: {total_articles}")
        
        return results

    async def save_news_to_db(self, news_item: NewsItem) -> bool:
        """ë‰´ìŠ¤ 1ê±´ì„ DBì— ì €ì¥"""
        try:
            # NewsItemì„ DB ì €ì¥ìš© ê°ì²´ë¡œ ë³€í™˜ (DatabaseManagerê°€ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœ)
            class DBNewsItem:
                def __init__(self, title, content, source_url, source, published_at, category, 
                           thumbnail=None, original_publish_date=None):
                    self.title = title
                    self.content = content
                    self.source_url = source_url
                    self.source = source
                    self.published_at = published_at
                    self.category = category
                    self.thumbnail = thumbnail
                    # ì‹¤ì œ ë°œí–‰ ì‹œê°„ í•„ë“œ ì¶”ê°€
                    self.publish_date = published_at  # í¬ë¡¤ë§ ì‹œê°„
                    self.original_publish_date = original_publish_date  # ì‹¤ì œ ë°œí–‰ ì‹œê°„
            
            logger.info(f"ğŸ” save_news_to_db - NewsItem í•„ë“œ í™•ì¸:")
            logger.info(f"   - publish_date: {news_item.publish_date} (í¬ë¡¤ë§ ì‹œê°„)")
            logger.info(f"   - original_publish_date: {news_item.original_publish_date} (ì‹¤ì œ ë°œí–‰ ì‹œê°„)")
            
            db_news_item = DBNewsItem(
                title=news_item.title,
                content=news_item.content,
                source_url=news_item.url,
                source=news_item.source,
                published_at=news_item.publish_date,
                category=news_item.category,
                thumbnail=news_item.thumbnail,
                original_publish_date=news_item.original_publish_date
            )
            
            logger.info(f"ğŸ” DB ì €ì¥ ì „ DBNewsItem í™•ì¸:")
            logger.info(f"   - publish_date: {db_news_item.publish_date}")
            logger.info(f"   - original_publish_date: {db_news_item.original_publish_date}")
            logger.info(f"   - ë‘ ì‹œê°„ì´ ê°™ì€ê°€? {db_news_item.publish_date == db_news_item.original_publish_date}")
            
            news_id = self.db_manager.save_news_item(db_news_item)
            if news_id:
                logger.info(f"âœ… DB ì €ì¥ ì„±ê³µ (ID: {news_id}): {news_item.title[:50]}...")
                return True
            else:
                logger.info(f"ğŸ”„ ì¤‘ë³µ ë‰´ìŠ¤ ìŠ¤í‚µ: {news_item.title[:50]}...")
                return False
            
        except Exception as e:
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    crawler = NaverMobileCrawler()
    
    try:
        print("ğŸš€ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° DB ì €ì¥ ì‹œì‘...")
        
        # í—¤ë“œë¼ì¸ ë‰´ìŠ¤ 10ê°œì”© í¬ë¡¤ë§
        results = await crawler.crawl_all_categories(10)
        
        total_crawled = sum(len(articles) for articles in results.values())
        print(f"\nğŸ¯ ë„¤ì´ë²„ í¬ë¡¤ë§ ì™„ë£Œ:")
        print(f"   ğŸ“Š ì´ í¬ë¡¤ë§: {total_crawled}ê°œ")
        
        print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼:")
        for category, articles in results.items():
            print(f"   {category}: {len(articles)}ê°œ")
            for article in articles[:1]:  # ì²« ë²ˆì§¸ë§Œ ì¶œë ¥
                print(f"     - {article.title[:60]}...")
                
        print(f"\nğŸ’¾ DB ì €ì¥: í¬ë¡¤ë§ê³¼ ë™ì‹œì— 1ê±´ì”© ì €ì¥ ì™„ë£Œ")
        print(f"   (ì¤‘ë³µ ë‰´ìŠ¤ëŠ” ìë™ìœ¼ë¡œ ìŠ¤í‚µë¨)")
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())