"""
í¬ë¡¤ë§ í†µí•© API ì—”ë“œí¬ì¸íŠ¸
- ìˆ˜ë™ í¬ë¡¤ë§ ì‹¤í–‰
- ìŠ¤ì¼€ì¤„ëŸ¬ ê´€ë¦¬
- í¬ë¡¤ë§ ìƒíƒœ ë° ë¡œê·¸ ì¡°íšŒ
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from typing import Dict, List, Optional
import logging
from datetime import datetime
import asyncio
import os
import sys
from pathlib import Path
import json
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from app.crawlers.crawler_manager import UnifiedCrawlerManager

router = APIRouter()
logger = logging.getLogger(__name__)

# ì „ì—­ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
manager_instance = None

def get_manager():
    global manager_instance
    if not manager_instance:
        database_url = os.getenv("DATABASE_URL", "postgresql://factlab_user:password@localhost:5433/factlab")
        manager_instance = UnifiedCrawlerManager(database_url)
    return manager_instance

@router.get("/health")
async def health_check():
    """API í—¬ìŠ¤ì²´í¬"""
    return {
        "status": "healthy",
        "service": "FactLab Unified Crawler API",
        "timestamp": datetime.now().isoformat()
    }

@router.post("/crawl/naver")
async def manual_crawl_naver():
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ë™ í¬ë¡¤ë§"""
    try:
        manager = get_manager()
        result = await manager.crawl_naver_news()
        
        return {
            "success": True,
            "message": "ë„¤ì´ë²„ í¬ë¡¤ë§ ì™„ë£Œ",
            **result
        }
        
    except Exception as e:
        logger.error(f"ë„¤ì´ë²„ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/crawl/daum")
async def manual_crawl_daum():
    """ë‹¤ìŒ ë‰´ìŠ¤ ìˆ˜ë™ í¬ë¡¤ë§"""
    try:
        manager = get_manager()
        result = await manager.crawl_daum_news()
        
        return {
            "success": True,
            "message": "ë‹¤ìŒ í¬ë¡¤ë§ ì™„ë£Œ",
            **result
        }
        
    except Exception as e:
        logger.error(f"ë‹¤ìŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ì „ì—­ í¬ë¡¤ë§ ìƒíƒœ ì €ì¥
crawl_progress = {
    "is_running": False,
    "total_articles": 0,
    "completed_articles": 0,
    "current_category": "",
    "current_source": "",
    "details": [],
    "error": None,
    "crawl_type": "none"  # "manual", "schedule", "none"
}

@router.post("/crawl/all")
async def manual_crawl_all():
    """ëª¨ë“  ì†ŒìŠ¤ ìˆ˜ë™ í¬ë¡¤ë§"""
    try:
        manager = get_manager()
        result = await manager.manual_crawl_all()
        
        return {
            "success": True,
            "message": "ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ",
            **result
        }
        
    except Exception as e:
        logger.error(f"ì „ì²´ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/crawl/progress")
async def get_crawl_progress():
    """í¬ë¡¤ë§ ì§„í–‰ë¥  ì¡°íšŒ"""
    return {
        "success": True,
        "progress": crawl_progress
    }

@router.get("/crawl/progress/stream")
async def stream_crawl_progress():
    """SSEë¡œ í¬ë¡¤ë§ ì§„í–‰ë¥  ìŠ¤íŠ¸ë¦¬ë°"""
    def generate():
        last_sent = {}
        while True:
            # ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ ì „ì†¡
            if crawl_progress != last_sent:
                data = json.dumps(crawl_progress)
                yield f"data: {data}\n\n"
                last_sent = crawl_progress.copy()
            
            # í¬ë¡¤ë§ì´ ì™„ë£Œë˜ë©´ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
            if not crawl_progress["is_running"] and crawl_progress["completed_articles"] > 0:
                break
                
            time.sleep(0.5)  # 0.5ì´ˆë§ˆë‹¤ ì²´í¬
    
    return StreamingResponse(
        generate(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*"
        }
    )

@router.get("/scheduler/status")
async def get_scheduler_status():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì¡°íšŒ"""
    try:
        manager = get_manager()
        status = manager.get_scheduler_status()
        return {
            "success": True,
            "scheduler": status,
        }
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/crawl/all/schedule")
async def schedule_crawl_all_with_progress(background_tasks: BackgroundTasks):
    """ìŠ¤ì¼€ì¤„ì— ì˜í•œ ìë™ í¬ë¡¤ë§ (ì¹´ë“œ ìƒíƒœ í‘œì‹œìš©)"""
    global crawl_progress
    
    if crawl_progress["is_running"]:
        raise HTTPException(status_code=400, detail="í¬ë¡¤ë§ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤")
    
    # í¬ë¡¤ë§ ìƒíƒœ ì´ˆê¸°í™” (ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§)
    crawl_progress.update({
        "is_running": True,
        "total_articles": 3,  # 3ê°œ ì†ŒìŠ¤ (ë„¤ì´ë²„, ë‹¤ìŒ, êµ¬ê¸€)
        "completed_articles": 0,
        "current_category": "",
        "current_source": "ë„¤ì´ë²„",
        "details": ["ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ ì¤€ë¹„ ì¤‘..."],
        "error": None,
        "crawl_type": "schedule"  # ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§
    })
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰
    background_tasks.add_task(run_real_crawl_with_progress)
    
    return {
        "success": True,
        "message": "ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
    }

@router.post("/crawl/all/stream")
async def manual_crawl_all_with_progress(background_tasks: BackgroundTasks):
    """ì‹¤ì‹œê°„ ì§„í–‰ë¥ ê³¼ í•¨ê»˜ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘"""
    print("=== MANUAL CRAWL ENDPOINT CALLED ===")
    logger.info("ğŸ¯ manual_crawl_all_with_progress ì—”ë“œí¬ì¸íŠ¸ ì‹œì‘!")
    global crawl_progress
    
    if crawl_progress["is_running"]:
        raise HTTPException(status_code=400, detail="í¬ë¡¤ë§ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤")
    
    # í¬ë¡¤ë§ ìƒíƒœ ì´ˆê¸°í™” (ìˆ˜ë™ í¬ë¡¤ë§)
    crawl_progress.update({
        "is_running": True,
        "total_articles": 3,  # 3ê°œ ì†ŒìŠ¤ (ë„¤ì´ë²„, ë‹¤ìŒ, êµ¬ê¸€)
        "completed_articles": 0,
        "current_category": "",
        "current_source": "ë„¤ì´ë²„",
        "details": ["í¬ë¡¤ë§ ì¤€ë¹„ ì¤‘..."],
        "error": None,
        "crawl_type": "manual"  # ìˆ˜ë™ í¬ë¡¤ë§
    })
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰ (ì‹¤ì œ í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € ì‚¬ìš©)
    logger.info("ğŸ¯ About to add run_real_crawl_with_progress to background_tasks")
    background_tasks.add_task(run_real_crawl_with_progress)
    logger.info("âœ… Added run_real_crawl_with_progress to background_tasks")
    
    return {
        "success": True,
        "message": "í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. /crawl/progress/streamì—ì„œ ì§„í–‰ë¥ ì„ í™•ì¸í•˜ì„¸ìš”."
    }

async def run_real_crawl_with_progress():
    """ì‹¤ì œ í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ë¥ ê³¼ í•¨ê»˜ í¬ë¡¤ë§ ì‹¤í–‰"""
    global crawl_progress
    
    try:
        logger.info("ğŸš€ run_real_crawl_with_progress í•¨ìˆ˜ ì‹œì‘!")
        logger.info(f"ğŸ” ì´ˆê¸° crawl_progress ìƒíƒœ: {crawl_progress}")
        manager = get_manager()
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜
        def progress_callback(source: str, category: str, completed: int, total: int, message: str):
            crawl_progress["current_source"] = source
            crawl_progress["current_category"] = category
            crawl_progress["completed_articles"] = completed
            crawl_progress["details"].append(message)
            
            # ìµœê·¼ 15ê°œ ë¡œê·¸ë§Œ ìœ ì§€
            if len(crawl_progress["details"]) > 15:
                crawl_progress["details"] = crawl_progress["details"][-15:]
        
        # ì‹¤ì œ í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì €ì˜ manual_crawl_all í˜¸ì¶œ
        logger.info("ğŸš€ ì‹¤ì œ í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì €ë¡œ í¬ë¡¤ë§ ì‹œì‘")
        crawl_progress["details"].append("ğŸš€ í¬ë¡¤ë§ ë§¤ë‹ˆì € ì‹œì‘...")
        
        # ê° ì†ŒìŠ¤ë³„ë¡œ í¬ë¡¤ë§ ì‹¤í–‰í•˜ë©° ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        total_saved = 0
        
        # ë„¤ì´ë²„ í¬ë¡¤ë§
        crawl_progress["current_source"] = "ë„¤ì´ë²„"
        crawl_progress["details"].append("ğŸ“± ë„¤ì´ë²„ ëª¨ë°”ì¼ í¬ë¡¤ë§ ì‹œì‘...")
        naver_result = await manager.crawl_naver_news()
        naver_saved = naver_result.get('total_saved', 0)
        total_saved += naver_saved
        crawl_progress["completed_articles"] = 1  # 1ê°œ ì†ŒìŠ¤ ì™„ë£Œ
        crawl_progress["details"].append(f"âœ… ë„¤ì´ë²„ ì™„ë£Œ: {naver_saved}ê°œ ì €ì¥")
        
        # ë‹¤ìŒ í¬ë¡¤ë§
        await asyncio.sleep(2)
        crawl_progress["current_source"] = "ë‹¤ìŒ"
        crawl_progress["details"].append("ğŸ“± ë‹¤ìŒ ëª¨ë°”ì¼ í¬ë¡¤ë§ ì‹œì‘...")
        daum_result = await manager.crawl_daum_news()
        daum_saved = daum_result.get('total_saved', 0)
        total_saved += daum_saved
        crawl_progress["completed_articles"] = 2  # 2ê°œ ì†ŒìŠ¤ ì™„ë£Œ
        crawl_progress["details"].append(f"âœ… ë‹¤ìŒ ì™„ë£Œ: {daum_saved}ê°œ ì €ì¥")
        
        # êµ¬ê¸€ í¬ë¡¤ë§
        await asyncio.sleep(2)
        crawl_progress["current_source"] = "êµ¬ê¸€"
        crawl_progress["details"].append("ğŸŒ êµ¬ê¸€ RSS í¬ë¡¤ë§ ì‹œì‘...")
        google_result = await manager.crawl_google_news()
        google_saved = google_result.get('total_saved', 0)
        total_saved += google_saved
        crawl_progress["completed_articles"] = 3  # 3ê°œ ì†ŒìŠ¤ ì™„ë£Œ
        crawl_progress["details"].append(f"âœ… êµ¬ê¸€ ì™„ë£Œ: {google_saved}ê°œ ì €ì¥")
        
        # í¬ë¡¤ë§ ì™„ë£Œ
        crawl_progress.update({
            "is_running": False,
            "completed_articles": total_saved,
            "current_category": "ì™„ë£Œ",
            "current_source": "ì™„ë£Œ",
            "details": crawl_progress["details"] + [f"ğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {total_saved}ê°œ ë‰´ìŠ¤ ì €ì¥"],
            "crawl_type": "none"  # í¬ë¡¤ë§ ì¢…ë£Œ
        })
        
        logger.info(f"âœ¨ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {total_saved}ê°œ ë‰´ìŠ¤ ì €ì¥")
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        crawl_progress.update({
            "is_running": False,
            "error": str(e),
            "details": crawl_progress["details"] + [f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"],
            "crawl_type": "none"  # í¬ë¡¤ë§ ì¢…ë£Œ
        })

# ê¸°ì¡´ ì‹œë®¬ë ˆì´ì…˜ í•¨ìˆ˜ëŠ” ìœ ì§€ (í…ŒìŠ¤íŠ¸ìš©)
async def run_crawl_with_progress_DISABLED():
    """ì§„í–‰ë¥ ì„ ì—…ë°ì´íŠ¸í•˜ë©´ì„œ í¬ë¡¤ë§ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)"""
    global crawl_progress
    
    try:
        manager = get_manager()
        
        # ë„¤ì´ë²„ í¬ë¡¤ë§
        from app.crawlers.naver_crawler import NaverMobileCrawler
        naver_crawler = NaverMobileCrawler()
        
        categories = ["politics", "economy", "society", "technology", "world", "sports"]
        category_names = {
            "politics": "ì •ì¹˜",
            "economy": "ê²½ì œ", 
            "society": "ì‚¬íšŒ",
            "technology": "IT/ê³¼í•™",
            "world": "ì„¸ê³„",
            "sports": "ìŠ¤í¬ì¸ "
        }
        
        total_saved = 0
        
        for i, category in enumerate(categories):
            category_name = category_names.get(category, category)
            crawl_progress["current_category"] = category_name
            crawl_progress["details"].append(f"ğŸ“‚ {category_name} ë¶„ì•¼ í¬ë¡¤ë§ ì‹œì‘...")
            
            try:
                # ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰
                news_items = await naver_crawler.crawl_category(category, 20)
                
                # ê° ë‰´ìŠ¤ ì•„ì´í…œ ì €ì¥ ì‹œë®¬ë ˆì´ì…˜
                saved_count = 0
                for j, news_item in enumerate(news_items):
                    try:
                        # ì‹¤ì œ DB ì €ì¥
                        success = await naver_crawler.save_news_to_db(news_item)
                        if success:
                            saved_count += 1
                            total_saved += 1
                            
                        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                        crawl_progress["completed_articles"] = total_saved
                        
                        # ê° ë‰´ìŠ¤ë§ˆë‹¤ ë¡œê·¸ ì¶”ê°€ (ìš”ì²­ëœ í˜•ì‹: ë²ˆí˜¸-ì œëª©-ì¶œì²˜)
                        if success:
                            news_log = f"ğŸ“° {total_saved}. {news_item.get('title', 'ì œëª©ì—†ìŒ')[:50]}{'...' if len(news_item.get('title', '')) > 50 else ''} - {news_item.get('source', 'ë„¤ì´ë²„')}"
                            crawl_progress["details"].append(news_log)
                            
                            # ìµœê·¼ 10ê°œ ë¡œê·¸ë§Œ ìœ ì§€
                            if len(crawl_progress["details"]) > 10:
                                crawl_progress["details"] = crawl_progress["details"][-10:]
                        
                        # 5ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ìš”ì•½
                        if (j + 1) % 5 == 0:
                            crawl_progress["details"].append(
                                f"âœ… {category_name} ë¶„ì•¼ {j + 1}ê°œ ì²˜ë¦¬ ì™„ë£Œ (ì €ì¥: {saved_count}ê°œ)"
                            )
                            # ìµœê·¼ 15ê°œ ë¡œê·¸ë§Œ ìœ ì§€ (ë‰´ìŠ¤ ë¡œê·¸ + ìš”ì•½ ë¡œê·¸)
                            if len(crawl_progress["details"]) > 15:
                                crawl_progress["details"] = crawl_progress["details"][-15:]
                        
                        # ì‹¤ì œ í¬ë¡¤ë§ ê°„ê²© (5ì´ˆ)
                        await asyncio.sleep(0.1)  # ë°ëª¨ìš©ìœ¼ë¡œ ì§§ê²Œ
                        
                    except Exception as e:
                        logger.error(f"ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                        continue
                
                crawl_progress["details"].append(
                    f"ğŸ¯ {category_name} ë¶„ì•¼ ì™„ë£Œ: {saved_count}ê°œ ì €ì¥"
                )
                
            except Exception as e:
                logger.error(f"{category} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                crawl_progress["details"].append(f"âŒ {category_name} ë¶„ì•¼ ì‹¤íŒ¨: {str(e)}")
                continue
        
        # í¬ë¡¤ë§ ì™„ë£Œ
        crawl_progress.update({
            "is_running": False,
            "completed_articles": total_saved,
            "current_category": "ì™„ë£Œ",
            "details": crawl_progress["details"] + [f"âœ¨ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {total_saved}ê°œ ë‰´ìŠ¤ ì €ì¥"]
        })
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        crawl_progress.update({
            "is_running": False,
            "error": str(e),
            "details": crawl_progress["details"] + [f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"],
            "crawl_type": "none"  # í¬ë¡¤ë§ ì¢…ë£Œ
        })


@router.post("/scheduler/start")
async def start_scheduler(background_tasks: BackgroundTasks):
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
    try:
        manager = get_manager()
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
        background_tasks.add_task(manager.start_scheduler)
        
        return {
            "success": True,
            "message": "í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
            "schedule": {
                "naver": "2ì‹œê°„ ê°„ê²© ì •ì‹œ (00:00, 02:00, 04:00, ...)",
                "daum": "2ì‹œê°„ ê°„ê²© 20ë¶„ (00:20, 02:20, 04:20, ...)",
                "google": "2ì‹œê°„ ê°„ê²© 40ë¶„ (00:40, 02:40, 04:40, ...)"
            }
        }
        
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/scheduler/stop")
async def stop_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
    try:
        manager = get_manager()
        await manager.stop_scheduler()
        
        return {
            "success": True,
            "message": "í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/logs")
async def get_crawl_logs(limit: int = 50):
    """í¬ë¡¤ë§ ë¡œê·¸ ì¡°íšŒ"""
    try:
        manager = get_manager()
        
        if not manager.database_url:
            raise HTTPException(status_code=500, detail="Database URL not configured")
        
        import psycopg2
        from psycopg2.extras import RealDictCursor
        
        conn = psycopg2.connect(manager.database_url, cursor_factory=RealDictCursor)
        
        try:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT * FROM crawl_logs 
                ORDER BY created_at DESC 
                LIMIT %s
            """, (limit,))
            
            logs = cursor.fetchall()
            
            return {
                "success": True,
                "logs": [dict(log) for log in logs],
                "count": len(logs)
            }
            
        finally:
            cursor.close()
            conn.close()
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ë¡œê·¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/stats")
async def get_crawl_stats():
    """í¬ë¡¤ë§ í†µê³„ ì¡°íšŒ"""
    try:
        manager = get_manager()
        
        if not manager.database_url:
            raise HTTPException(status_code=500, detail="Database URL not configured")
        
        import psycopg2
        from psycopg2.extras import RealDictCursor
        
        conn = psycopg2.connect(manager.database_url, cursor_factory=RealDictCursor)
        
        try:
            cursor = conn.cursor()
            
            # ì˜¤ëŠ˜ í¬ë¡¤ë§ í†µê³„
            cursor.execute("""
                SELECT source, 
                       COUNT(*) as crawl_count,
                       SUM(articles_saved) as total_articles,
                       AVG(duration_seconds) as avg_duration,
                       MAX(created_at) as last_crawl
                FROM crawl_logs 
                WHERE DATE(created_at) = CURRENT_DATE
                GROUP BY source
                ORDER BY total_articles DESC
            """)
            
            today_stats = cursor.fetchall()
            
            # ì£¼ê°„ í†µê³„
            cursor.execute("""
                SELECT source,
                       COUNT(*) as crawl_count,
                       SUM(articles_saved) as total_articles,
                       AVG(duration_seconds) as avg_duration
                FROM crawl_logs 
                WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
                GROUP BY source
                ORDER BY total_articles DESC
            """)
            
            weekly_stats = cursor.fetchall()
            
            # ì „ì²´ ë‰´ìŠ¤ ìˆ˜
            cursor.execute("""
                SELECT COUNT(*) as total_news,
                       COUNT(CASE WHEN status = 'PENDING' THEN 1 END) as pending_news,
                       COUNT(CASE WHEN status = 'APPROVED' THEN 1 END) as approved_news,
                       COUNT(CASE WHEN status = 'REJECTED' THEN 1 END) as rejected_news
                FROM news
            """)
            
            news_stats = cursor.fetchone()
            
            # ì†ŒìŠ¤ë³„ ë‰´ìŠ¤ ìˆ˜
            cursor.execute("""
                SELECT source, COUNT(*) as count
                FROM news
                GROUP BY source
                ORDER BY count DESC
            """)
            
            source_stats = cursor.fetchall()
            
            return {
                "success": True,
                "today": [dict(stat) for stat in today_stats],
                "weekly": [dict(stat) for stat in weekly_stats],
                "news_total": dict(news_stats),
                "by_source": [dict(stat) for stat in source_stats]
            }
            
        finally:
            cursor.close()
            conn.close()
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/test/single")
async def test_single_source(source: str, count: int = 3):
    """ë‹¨ì¼ ì†ŒìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    try:
        manager = get_manager()
        
        if source.lower() == "naver":
            # ë„¤ì´ë²„ ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í…ŒìŠ¤íŠ¸
            result = await manager.naver_crawler.crawl_category("politics", count)
            items = [
                {
                    "title": item.title,
                    "url": item.url,
                    "content_length": len(item.content),
                    "category": item.category
                } for item in result
            ]
            
        elif source.lower() == "daum":
            # ë‹¤ìŒ ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í…ŒìŠ¤íŠ¸
            result = await manager.daum_crawler.crawl_category("politics", count)
            items = [
                {
                    "title": item.title,
                    "url": item.url,
                    "content_length": len(item.content),
                    "category": item.category
                } for item in result
            ]
            
        elif source.lower() == "google":
            # êµ¬ê¸€ ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í…ŒìŠ¤íŠ¸
            result = manager.google_crawler.crawl_rss("politics")[:count]
            items = [
                {
                    "title": item.title,
                    "url": item.source_url,
                    "content_length": len(item.content),
                    "category": item.category
                } for item in result
            ]
            
        else:
            raise HTTPException(status_code=400, detail="Invalid source. Use 'naver', 'daum', or 'google'")
        
        return {
            "success": True,
            "source": source,
            "crawled": len(items),
            "items": items
        }
        
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/config")
async def get_crawler_config():
    """í¬ë¡¤ëŸ¬ ì„¤ì • ì¡°íšŒ"""
    try:
        manager = get_manager()
        
        return {
            "success": True,
            "config": {
                "articles_per_category": manager.articles_per_category,
                "crawl_interval_minutes": manager.crawl_interval_minutes,
                "supported_categories": manager.categories,
                "sources": ["naver", "daum", "google"],
                "schedule": {
                    "naver": "2ì‹œê°„ ê°„ê²© ì •ì‹œ (00:00, 02:00, 04:00, ...)",
                    "daum": "2ì‹œê°„ ê°„ê²© 20ë¶„ (00:20, 02:20, 04:20, ...)",
                    "google": "2ì‹œê°„ ê°„ê²© 40ë¶„ (00:40, 02:40, 04:40, ...)"
                }
            }
        }
        
    except Exception as e:
        logger.error(f"ì„¤ì • ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))