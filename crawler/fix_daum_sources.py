#!/usr/bin/env python3
"""
ë‹¤ìŒ ë‰´ìŠ¤ source ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸
ê¸°ì¡´ì— "ì•Œìˆ˜ì—†ìŒ"ìœ¼ë¡œ ì €ì¥ëœ ë‹¤ìŒ ë‰´ìŠ¤ë“¤ì˜ sourceë¥¼ ì‹¤ì œë¡œ ì¶”ì¶œí•´ì„œ ì—…ë°ì´íŠ¸
"""

import asyncio
import aiohttp
import logging
from bs4 import BeautifulSoup
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from app.database.db_manager import DatabaseManager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DaumSourceFixer:
    def __init__(self):
        self.db_manager = DatabaseManager()
        self.session = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://m.media.daum.net/'
        }
    
    async def init_session(self):
        """HTTP ì„¸ì…˜ ì´ˆê¸°í™”"""
        if not self.session:
            timeout = aiohttp.ClientTimeout(total=30, connect=10)
            connector = aiohttp.TCPConnector(
                limit=10, 
                limit_per_host=5,
                ssl=False  # SSL ê²€ì¦ ë¹„í™œì„±í™”
            )
            self.session = aiohttp.ClientSession(
                headers=self.headers,
                timeout=timeout,
                connector=connector
            )
            logger.info("HTTP ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def close_session(self):
        """HTTP ì„¸ì…˜ ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
            self.session = None
            logger.info("HTTP ì„¸ì…˜ ì¢…ë£Œ")
    
    def extract_news_source(self, soup: BeautifulSoup) -> str:
        """ë‹¤ìŒ ë‰´ìŠ¤ì—ì„œ ì‹¤ì œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ (ìˆ˜ì •ëœ ë¡œì§ ì ìš©)"""
        try:
            # 1ìˆœìœ„: og:article:author ë©”íƒ€íƒœê·¸
            meta_og_author = soup.find('meta', property='og:article:author')
            if meta_og_author and meta_og_author.get('content'):
                source = meta_og_author['content'].strip()
                if source and len(source) < 30:
                    logger.info(f"âœ… og:article:authorì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ: {source}")
                    return source
            
            # 2ìˆœìœ„: og:site_nameì—ì„œ ì¶”ì¶œ (ë‹¤ìŒ - ì–¸ë¡ ì‚¬ëª… í˜•íƒœ)
            meta_og_site_name = soup.find('meta', property='og:site_name')
            if meta_og_site_name and meta_og_site_name.get('content'):
                site_name = meta_og_site_name['content'].strip()
                if site_name and 'ë‹¤ìŒ -' in site_name:
                    # "ë‹¤ìŒ - í—¤ëŸ´ë“œê²½ì œ" í˜•íƒœì—ì„œ ì–¸ë¡ ì‚¬ëª…ë§Œ ì¶”ì¶œ
                    source = site_name.split('ë‹¤ìŒ -')[-1].strip()
                    if source and len(source) < 30:
                        logger.info(f"âœ… og:site_nameì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ: {source}")
                        return source
                        
            # 3ìˆœìœ„: ê¸°ì¡´ article:authorë„ í™•ì¸ (í˜¸í™˜ì„±)
            meta_source = soup.find('meta', property='article:author')
            if meta_source and meta_source.get('content'):
                source = meta_source['content'].strip()
                if source and len(source) < 30:
                    logger.info(f"âœ… article:authorì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ: {source}")
                    return source

            # 4ìˆœìœ„: ë‹¤ìŒ ë‰´ìŠ¤ êµ¬ì¡°ì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ
            source_selectors = [
                '.info_news .txt_info',  # ë‹¤ìŒ ë‰´ìŠ¤ ì •ë³´ ì˜ì—­
                '.head_view .info_news .link_cp',  # ì–¸ë¡ ì‚¬ ë§í¬
                '.info_news .txt_cp',  # ì–¸ë¡ ì‚¬ëª…
                '.news_cp img',  # ì–¸ë¡ ì‚¬ ë¡œê³  ì´ë¯¸ì§€ì˜ alt
                '.cp_name',  # ì–¸ë¡ ì‚¬ëª… í´ë˜ìŠ¤
                '.source',  # ì¶œì²˜
                '.media_cp_name',  # ë¯¸ë””ì–´ ì–¸ë¡ ì‚¬ëª…
            ]
            
            for selector in source_selectors:
                elements = soup.select(selector)
                for element in elements:
                    # img íƒœê·¸ì˜ alt ì†ì„±ì—ì„œ ì¶”ì¶œ
                    if element.name == 'img':
                        alt_text = element.get('alt', '').strip()
                        if alt_text and alt_text not in ['logo', 'icon', 'ë¡œê³ ']:
                            logger.info(f"âœ… ì´ë¯¸ì§€ altì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ: {alt_text}")
                            return alt_text
                    
                    # í…ìŠ¤íŠ¸ ìš”ì†Œì—ì„œ ì¶”ì¶œ
                    text = element.get_text().strip()
                    if text and len(text) > 1 and len(text) < 20:
                        logger.info(f"âœ… í…ìŠ¤íŠ¸ì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ: {text}")
                        return text
                        
                    # ë§í¬ì˜ title ì†ì„±ì—ì„œ ì¶”ì¶œ
                    title_text = element.get('title', '').strip()
                    if title_text and 'ì–¸ë¡ ì‚¬' not in title_text and len(title_text) < 20:
                        logger.info(f"âœ… title ì†ì„±ì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ: {title_text}")
                        return title_text
            
            # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ìœ ì§€
            logger.warning("âš ï¸ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ ì‹¤íŒ¨")
            return "ì•Œìˆ˜ì—†ìŒ"
            
        except Exception as e:
            logger.error(f"âŒ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return "ì•Œìˆ˜ì—†ìŒ"
    
    async def fetch_news_source(self, url: str) -> str:
        """ê°œë³„ ë‰´ìŠ¤ URLì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ"""
        try:
            await asyncio.sleep(2)  # ìš”ì²­ ê°„ê²©
            
            async with self.session.get(url) as response:
                if response.status != 200:
                    logger.error(f"HTTP {response.status}: {url}")
                    return "ì•Œìˆ˜ì—†ìŒ"
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                source = self.extract_news_source(soup)
                return source
                
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ì†ŒìŠ¤ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
            return "ì•Œìˆ˜ì—†ìŒ"
    
    async def fix_daum_sources(self):
        """ë‹¤ìŒ ë‰´ìŠ¤ source ì¼ê´„ ìˆ˜ì •"""
        logger.info("ğŸ”§ ë‹¤ìŒ ë‰´ìŠ¤ source ìˆ˜ì • ì‹œì‘...")
        
        # 1. "ì•Œìˆ˜ì—†ìŒ"ìœ¼ë¡œ ëœ ë‹¤ìŒ ë‰´ìŠ¤ë“¤ ì¡°íšŒ
        daum_news = self.db_manager.get_daum_news_with_unknown_source()
        
        if not daum_news:
            logger.info("ìˆ˜ì •í•  ë‹¤ìŒ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ìˆ˜ì • ëŒ€ìƒ: {len(daum_news)}ê°œ ë‹¤ìŒ ë‰´ìŠ¤")
        
        await self.init_session()
        
        updated_count = 0
        
        try:
            for news in daum_news:
                news_id = news['id']
                url = news['url']
                current_source = news.get('source', 'ì•Œìˆ˜ì—†ìŒ')
                
                logger.info(f"ìˆ˜ì • ì¤‘: [{news_id}] {url}")
                
                # ì‹¤ì œ URLì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ
                new_source = await self.fetch_news_source(url)
                
                if new_source and new_source != "ì•Œìˆ˜ì—†ìŒ" and new_source != current_source:
                    # DB ì—…ë°ì´íŠ¸
                    success = self.db_manager.update_news_source(news_id, new_source)
                    if success:
                        updated_count += 1
                        logger.info(f"âœ… ì—…ë°ì´íŠ¸ ì™„ë£Œ: [{news_id}] {current_source} â†’ {new_source}")
                    else:
                        logger.error(f"âŒ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: [{news_id}]")
                else:
                    logger.warning(f"âš ï¸ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë³€ê²½ ì—†ìŒ: [{news_id}] {new_source}")
        
        finally:
            await self.close_session()
        
        logger.info(f"ğŸ¯ ë‹¤ìŒ ë‰´ìŠ¤ source ìˆ˜ì • ì™„ë£Œ: {updated_count}/{len(daum_news)}ê°œ ì—…ë°ì´íŠ¸")

async def main():
    fixer = DaumSourceFixer()
    await fixer.fix_daum_sources()

if __name__ == "__main__":
    asyncio.run(main())